#!/usr/bin/env python3
"""
åˆ†æ‰¹æ–‡æ¡£ç´¢å¼•å™¨
æ”¯æŒå¤§è§„æ¨¡æ–‡æ¡£çš„åˆ†æ‰¹ç´¢å¼•å¤„ç†ï¼Œæä¾›è¿›åº¦è·Ÿè¸ªã€æ–­ç‚¹ç»­ä¼ å’Œå†…å­˜ä¼˜åŒ–
"""

import os
import sys
import json
import asyncio
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
from dataclasses import dataclass, asdict

# æ·»åŠ è„šæœ¬ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from document_processor import DocumentProcessorFactory
from simple_vectorizer import SimpleDocumentVectorizer
from config_loader import Context7ConfigLoader


@dataclass
class IndexProgress:
    """ç´¢å¼•è¿›åº¦è·Ÿè¸ª"""
    total_files: int
    processed_files: int
    success_count: int
    failed_files: List[str]
    skipped_files: List[str]
    start_time: str
    last_update: str
    checkpoint: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'IndexProgress':
        return cls(**data)


@dataclass
class BatchConfig:
    """åˆ†æ‰¹é…ç½®"""
    batch_size: int = 50  # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
    max_concurrent: int = 5  # å¹¶å‘å¤„ç†æ•°
    enable_checkpoint: bool = True  # å¯ç”¨æ–­ç‚¹ç»­ä¼ 
    checkpoint_interval: int = 10  # æ¯å¤„ç† N ä¸ªæ–‡æ¡£ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    memory_limit_mb: int = 1024  # å†…å­˜é™åˆ¶ï¼ˆMBï¼‰
    skip_indexed: bool = True  # è·³è¿‡å·²ç´¢å¼•çš„æ–‡æ¡£


class BatchDocumentIndexer:
    """åˆ†æ‰¹æ–‡æ¡£ç´¢å¼•å™¨"""

    def __init__(self, config: Optional[BatchConfig] = None):
        self.config = config or BatchConfig()
        self.config_loader = Context7ConfigLoader()
        self.doc_processor_factory = DocumentProcessorFactory()

        # è·å–æ’ä»¶æ ¹ç›®å½•
        self.plugin_root = Path(os.environ.get('CLAUDE_PLUGIN_ROOT',
                                             script_dir.parent.parent.parent))

        # å†…ç½®æ–‡æ¡£è·¯å¾„
        self.builtin_docs_path = self.plugin_root / "docs"

        # æ•°æ®ç›®å½•
        self.data_dir = Path(self.config_loader.load_config().get('cache', {}).get(
            'storage_path', str(script_dir.parent / "data")))
        self.checkpoint_file = self.data_dir / "index_checkpoint.json"

        # åˆå§‹åŒ–å‘é‡åŒ–å¼•æ“
        self.vectorizer = SimpleDocumentVectorizer(data_dir=str(self.data_dir))

        # å·²ç´¢å¼•æ–‡æ¡£é›†åˆï¼ˆç”¨äºå»é‡ï¼‰
        self._indexed_docs: Set[str] = set()

    async def get_indexed_documents(self) -> Set[str]:
        """è·å–å·²ç´¢å¼•æ–‡æ¡£IDé›†åˆ"""
        try:
            documents = await self.vectorizer.list_documents("builtin")
            return {doc['doc_id'] for doc in documents}
        except Exception:
            return set()

    def _generate_doc_id(self, file_path: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£IDï¼ˆç”¨äºå»é‡æ£€æŸ¥ï¼‰"""
        file_stat = os.stat(file_path)
        content_hash = hashlib.md5(
            f"{file_path}:{file_stat.st_size}:{file_stat.st_mtime}".encode()
        ).hexdigest()[:12]
        return f"builtin_doc_{content_hash}"

    async def index_all_docs(self, docs_path: Optional[Path] = None) -> IndexProgress:
        """ç´¢å¼•æ‰€æœ‰æ–‡æ¡£ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰"""
        target_path = docs_path or self.builtin_docs_path

        print(f"ğŸš€ å¼€å§‹åˆ†æ‰¹ç´¢å¼•æ–‡æ¡£...")
        print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_path}")
        print(f"âš™ï¸  é…ç½®: æ‰¹æ¬¡å¤§å°={self.config.batch_size}, å¹¶å‘æ•°={self.config.max_concurrent}")

        if not target_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_path}")
            return IndexProgress(
                total_files=0,
                processed_files=0,
                success_count=0,
                failed_files=[],
                skipped_files=[],
                start_time=datetime.now().isoformat(),
                last_update=datetime.now().isoformat()
            )

        # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
        doc_files = self._find_documents(target_path)

        if not doc_files:
            print("âŒ æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
            return IndexProgress(
                total_files=0,
                processed_files=0,
                success_count=0,
                failed_files=[],
                skipped_files=[],
                start_time=datetime.now().isoformat(),
                last_update=datetime.now().isoformat()
            )

        # åŠ è½½è¿›åº¦ï¼ˆå¦‚æœå¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼‰
        progress = await self._load_checkpoint(doc_files)

        print(f"ğŸ“„ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")

        # è·å–å·²ç´¢å¼•æ–‡æ¡£ï¼ˆç”¨äºå»é‡ï¼‰
        if self.config.skip_indexed:
            self._indexed_docs = await self.get_indexed_documents()
            print(f"âœ… å·²ç´¢å¼• {len(self._indexed_docs)} ä¸ªæ–‡æ¡£")

        # åˆ†æ‰¹å¤„ç†
        await self._process_batches(doc_files, progress)

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        await self._save_checkpoint(progress)

        # è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
        self._print_summary(progress)

        return progress

    def _find_documents(self, docs_path: Path) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶"""
        supported_extensions = []
        for processor in self.doc_processor_factory.processors.values():
            supported_extensions.extend(processor.supported_extensions())

        doc_files = []
        for ext in supported_extensions:
            doc_files.extend(docs_path.rglob(f"*{ext}"))

        return sorted(set(doc_files))

    async def _process_batches(self, doc_files: List[Path], progress: IndexProgress):
        """åˆ†æ‰¹å¤„ç†æ–‡æ¡£"""
        total_batches = (len(doc_files) + self.config.batch_size - 1) // self.config.batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * self.config.batch_size
            end_idx = min(start_idx + self.config.batch_size, len(doc_files))
            batch_files = doc_files[start_idx:end_idx]

            print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_files)} ä¸ªæ–‡ä»¶)")

            await self._process_batch(batch_files, progress)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.config.enable_checkpoint and progress.processed_files % self.config.checkpoint_interval == 0:
                await self._save_checkpoint(progress)

    async def _process_batch(self, batch_files: List[Path], progress: IndexProgress):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.config.max_concurrent)

        async def process_with_semaphore(doc_file: Path):
            async with semaphore:
                return await self._process_document(doc_file, progress)

        # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
        tasks = [process_with_semaphore(doc_file) for doc_file in batch_files]
        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_document(self, doc_file: Path, progress: IndexProgress) -> bool:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = self._generate_doc_id(str(doc_file))

            # æ£€æŸ¥æ˜¯å¦å·²ç´¢å¼•
            if self.config.skip_indexed and doc_id in self._indexed_docs:
                progress.skipped_files.append(str(doc_file))
                progress.processed_files += 1
                return True

            # å¤„ç†æ–‡æ¡£
            document = await self.doc_processor_factory.process_document(str(doc_file))
            if not document:
                progress.skipped_files.append(str(doc_file))
                progress.processed_files += 1
                return False

            # æ ‡è®°ä¸ºå†…ç½®æ–‡æ¡£
            document['metadata']['source'] = 'builtin'
            document['metadata']['category'] = self._categorize_doc(str(doc_file))
            document['doc_id'] = doc_id

            # å‘é‡åŒ–å­˜å‚¨
            await self.vectorizer.vectorize_and_store(document)

            progress.success_count += 1
            progress.processed_files += 1
            progress.last_update = datetime.now().isoformat()

            # è¾“å‡ºè¿›åº¦
            if progress.processed_files % 5 == 0:
                self._print_progress(progress)

            return True

        except Exception as e:
            error_msg = f"{doc_file}: {str(e)}"
            progress.failed_files.append(error_msg)
            progress.processed_files += 1
            print(f"âŒ å¤„ç†å¤±è´¥: {doc_file.name} - {e}")
            return False

    def _categorize_doc(self, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šæ–‡æ¡£åˆ†ç±»"""
        path_obj = Path(file_path)
        relative_path = path_obj.relative_to(self.builtin_docs_path)

        if 'guides' in str(relative_path):
            return 'guide'
        elif 'rules' in str(relative_path):
            return 'standards'
        elif 'templates' in str(relative_path):
            return 'template'
        elif 'api' in str(relative_path):
            return 'api'
        else:
            return 'general'

    async def _load_checkpoint(self, doc_files: List[Path]) -> IndexProgress:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not self.config.enable_checkpoint or not self.checkpoint_file.exists():
            return IndexProgress(
                total_files=len(doc_files),
                processed_files=0,
                success_count=0,
                failed_files=[],
                skipped_files=[],
                start_time=datetime.now().isoformat(),
                last_update=datetime.now().isoformat()
            )

        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                progress = IndexProgress.from_dict(data)
                progress.total_files = len(doc_files)
                print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: å·²å¤„ç† {progress.processed_files}/{len(doc_files)} ä¸ªæ–‡æ¡£")
                return progress
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹: {e}")
            return IndexProgress(
                total_files=len(doc_files),
                processed_files=0,
                success_count=0,
                failed_files=[],
                skipped_files=[],
                start_time=datetime.now().isoformat(),
                last_update=datetime.now().isoformat()
            )

    async def _save_checkpoint(self, progress: IndexProgress):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if not self.config.enable_checkpoint:
            return

        try:
            self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(progress.to_dict(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def _print_progress(self, progress: IndexProgress):
        """æ‰“å°è¿›åº¦"""
        percentage = (progress.processed_files / progress.total_files) * 100
        print(f"   è¿›åº¦: {progress.processed_files}/{progress.total_files} ({percentage:.1f}%) | "
              f"âœ… {progress.success_count} | âŒ {len(progress.failed_files)} | â­ï¸ {len(progress.skipped_files)}")

    def _print_summary(self, progress: IndexProgress):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print(f"\nğŸ“Š ç´¢å¼•å®Œæˆç»Ÿè®¡")
        print(f"   æ€»æ–‡æ¡£æ•°: {progress.total_files}")
        print(f"   å·²å¤„ç†: {progress.processed_files}")
        print(f"   âœ… æˆåŠŸ: {progress.success_count}")
        print(f"   â­ï¸  è·³è¿‡: {len(progress.skipped_files)}")
        print(f"   âŒ å¤±è´¥: {len(progress.failed_files)}")

        if progress.failed_files:
            print(f"\nâŒ å¤±è´¥æ–‡æ¡£åˆ—è¡¨:")
            for error in progress.failed_files[:10]:
                print(f"   - {error}")
            if len(progress.failed_files) > 10:
                print(f"   ... è¿˜æœ‰ {len(progress.failed_files) - 10} ä¸ªå¤±è´¥")

    async def list_indexed_documents(self):
        """åˆ—å‡ºå·²ç´¢å¼•çš„æ–‡æ¡£"""
        print("\nğŸ“‹ å·²ç´¢å¼•çš„æ–‡æ¡£:")

        try:
            documents = await self.vectorizer.list_documents("builtin")

            if not documents:
                print("   æš‚æ— å·²ç´¢å¼•çš„æ–‡æ¡£")
                return

            # æŒ‰åˆ†ç±»åˆ†ç»„
            by_category: Dict[str, List[Dict]] = {}
            for doc in documents:
                category = doc.get('metadata', {}).get('category', 'general')
                if category not in by_category:
                    by_category[category] = []
                by_category[category].append(doc)

            # æ‰“å°åˆ†ç»„ç»“æœ
            for category, docs in sorted(by_category.items()):
                print(f"\n   ğŸ“‚ {category.upper()} ({len(docs)} ä¸ªæ–‡æ¡£):")
                for doc in docs[:5]:  # æ¯ä¸ªåˆ†ç±»æœ€å¤šæ˜¾ç¤º5ä¸ª
                    print(f"      - ğŸ“„ {doc['title']} ({doc.get('chunk_count', 0)} å—)")
                if len(docs) > 5:
                    print(f"      ... è¿˜æœ‰ {len(docs) - 5} ä¸ªæ–‡æ¡£")

        except Exception as e:
            print(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")

    async def clear_index(self):
        """æ¸…é™¤æ‰€æœ‰ç´¢å¼•"""
        print("âš ï¸  è­¦å‘Š: æ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡æ¡£")
        confirm = input("ç¡®è®¤æ¸…é™¤? (yes/no): ")

        if confirm.lower() != 'yes':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return

        try:
            # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()

            # åˆ é™¤æ•°æ®åº“æ–‡ä»¶
            for db_file in self.data_dir.glob("*.db"):
                db_file.unlink()
                print(f"ğŸ—‘ï¸  å·²åˆ é™¤: {db_file.name}")

            print("âœ… ç´¢å¼•å·²æ¸…é™¤")

        except Exception as e:
            print(f"âŒ æ¸…é™¤ç´¢å¼•å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="åˆ†æ‰¹æ–‡æ¡£ç´¢å¼•å™¨")
    parser.add_argument('--batch-size', type=int, default=50, help='æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡')
    parser.add_argument('--concurrent', type=int, default=5, help='å¹¶å‘å¤„ç†æ•°')
    parser.add_argument('--no-checkpoint', action='store_true', help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reindex', action='store_true', help='é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡æ¡£')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºå·²ç´¢å¼•çš„æ–‡æ¡£')
    parser.add_argument('--clear', action='store_true', help='æ¸…é™¤æ‰€æœ‰ç´¢å¼•')
    parser.add_argument('--path', type=str, help='æŒ‡å®šè¦ç´¢å¼•çš„ç›®å½•è·¯å¾„')

    args = parser.parse_args()

    try:
        # åˆ›å»ºé…ç½®
        config = BatchConfig(
            batch_size=args.batch_size,
            max_concurrent=args.concurrent,
            enable_checkpoint=not args.no_checkpoint,
            skip_indexed=not args.reindex
        )

        indexer = BatchDocumentIndexer(config)

        # åˆ—å‡ºå·²ç´¢å¼•æ–‡æ¡£
        if args.list:
            await indexer.list_indexed_documents()
            return

        # æ¸…é™¤ç´¢å¼•
        if args.clear:
            await indexer.clear_index()
            return

        # æ‰§è¡Œç´¢å¼•
        docs_path = Path(args.path) if args.path else None
        await indexer.index_all_docs(docs_path)

        # åˆ—å‡ºç´¢å¼•ç»“æœ
        await indexer.list_indexed_documents()

    except KeyboardInterrupt:
        print("\nâš ï¸  ç´¢å¼•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ç´¢å¼•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
