#!/usr/bin/env python3
"""
æ‰¹é‡æ–‡æ¡£è½½å…¥å™¨
æ”¯æŒåˆ†æ‰¹ä»å‘é‡æ•°æ®åº“è½½å…¥æ–‡æ¡£ï¼Œæä¾›å†…å­˜ä¼˜åŒ–å’Œæ™ºèƒ½ç¼“å­˜
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Callable
from dataclasses import dataclass
from datetime import datetime
import sqlite3
from contextlib import contextmanager

# æ·»åŠ è„šæœ¬ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from config_loader import Context7ConfigLoader


@dataclass
class LoadConfig:
    """æ‰¹é‡è½½å…¥é…ç½®"""
    batch_size: int = 20  # æ¯æ‰¹è½½å…¥çš„æ–‡æ¡£æ•°é‡
    enable_cache: bool = True  # å¯ç”¨ç¼“å­˜
    cache_size: int = 100  # ç¼“å­˜æ–‡æ¡£æ•°é‡
    filter_category: Optional[str] = None  # æŒ‰åˆ†ç±»è¿‡æ»¤
    filter_source: Optional[str] = None  # æŒ‰æ¥æºè¿‡æ»¤
    min_chunk_count: Optional[int] = None  # æœ€å°å—æ•°è¿‡æ»¤


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—"""
    chunk_id: str
    doc_id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None


@dataclass
class Document:
    """æ–‡æ¡£"""
    doc_id: str
    title: str
    content: str
    format: str
    file_path: str
    metadata: Dict[str, Any]
    chunks: List[DocumentChunk]
    chunk_count: int
    indexed_at: Optional[str] = None


class DocumentCache:
    """æ–‡æ¡£ç¼“å­˜"""

    def __init__(self, max_size: int = 100):
        self.max_size = max_size
        self._cache: Dict[str, Document] = {}
        self._access_order: List[str] = []

    def get(self, doc_id: str) -> Optional[Document]:
        """è·å–ç¼“å­˜çš„æ–‡æ¡£"""
        if doc_id in self._cache:
            # æ›´æ–°è®¿é—®é¡ºåº
            self._access_order.remove(doc_id)
            self._access_order.append(doc_id)
            return self._cache[doc_id]
        return None

    def put(self, doc: Document):
        """ç¼“å­˜æ–‡æ¡£"""
        if doc.doc_id in self._cache:
            # æ›´æ–°è®¿é—®é¡ºåº
            self._access_order.remove(doc.doc_id)
            self._access_order.append(doc.doc_id)
            self._cache[doc.doc_id] = doc
        else:
            # æ·»åŠ æ–°æ–‡æ¡£
            if len(self._cache) >= self.max_size:
                # LRU æ·˜æ±°
                oldest = self._access_order.pop(0)
                del self._cache[oldest]
            self._cache[doc.doc_id] = doc
            self._access_order.append(doc.doc_id)

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._access_order.clear()

    def size(self) -> int:
        """ç¼“å­˜å¤§å°"""
        return len(self._cache)


class BatchDocumentLoader:
    """æ‰¹é‡æ–‡æ¡£è½½å…¥å™¨"""

    def __init__(self, config: Optional[LoadConfig] = None):
        self.config = config or LoadConfig()
        self.config_loader = Context7ConfigLoader()

        # æ•°æ®ç›®å½•
        script_dir = Path(__file__).parent
        self.data_dir = Path(self.config_loader.load_config().get('cache', {}).get(
            'storage_path', str(script_dir.parent / "data")))

        # æ•°æ®åº“æ–‡ä»¶
        self.metadata_db = self.data_dir / "metadata.db"
        self.vectors_db = self.data_dir / "vectors.db"

        # ç¼“å­˜
        self.cache = DocumentCache(max_size=self.config.cache_size) if self.config.enable_cache else None

    @contextmanager
    def _get_db_connection(self, db_path: Path):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()

    async def list_documents(self, batch: int = 0, scope: str = "all") -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ–‡æ¡£ï¼ˆåˆ†æ‰¹ï¼‰"""
        if not self.metadata_db.exists():
            return []

        offset = batch * self.config.batch_size
        limit = self.config.batch_size

        with self._get_db_connection(self.metadata_db) as conn:
            # æ„å»ºæŸ¥è¯¢ï¼ˆæ ¹æ®å®é™…çš„æ•°æ®åº“æ¶æ„ï¼‰
            if self.config.filter_source or self.config.filter_category:
                query = """
                    SELECT doc_id, title, file_path, format, chunk_count, metadata, created_at, updated_at
                    FROM documents
                    WHERE 1=1
                """
                params = []

                # æ·»åŠ è¿‡æ»¤æ¡ä»¶
                if self.config.filter_source:
                    query += " AND json_extract(metadata, '$.source') = ?"
                    params.append(self.config.filter_source)
                if self.config.filter_category:
                    query += " AND json_extract(metadata, '$.category') = ?"
                    params.append(self.config.filter_category)

                # æŒ‰ scope è¿‡æ»¤
                if scope == "builtin":
                    query += " AND doc_id LIKE 'builtin_%'"
                elif scope == "user":
                    query += " AND doc_id NOT LIKE 'builtin_%'"

                query += " ORDER BY updated_at DESC LIMIT ? OFFSET ?"
                params.extend([limit, offset])

                cursor = conn.execute(query, params)
            else:
                if scope == "builtin":
                    cursor = conn.execute("""
                        SELECT doc_id, title, file_path, format, chunk_count, metadata, created_at, updated_at
                        FROM documents
                        WHERE doc_id LIKE 'builtin_%'
                        ORDER BY updated_at DESC
                        LIMIT ? OFFSET ?
                    """, (limit, offset))
                elif scope == "user":
                    cursor = conn.execute("""
                        SELECT doc_id, title, file_path, format, chunk_count, metadata, created_at, updated_at
                        FROM documents
                        WHERE doc_id NOT LIKE 'builtin_%'
                        ORDER BY updated_at DESC
                        LIMIT ? OFFSET ?
                    """, (limit, offset))
                else:
                    cursor = conn.execute("""
                        SELECT doc_id, title, file_path, format, chunk_count, metadata, created_at, updated_at
                        FROM documents
                        ORDER BY updated_at DESC
                        LIMIT ? OFFSET ?
                    """, (limit, offset))

            rows = cursor.fetchall()

            return [dict(row) for row in rows]

    async def load_document(self, doc_id: str) -> Optional[Document]:
        """è½½å…¥å•ä¸ªæ–‡æ¡£"""
        # æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached = self.cache.get(doc_id)
            if cached:
                return cached

        if not self.metadata_db.exists():
            return None

        with self._get_db_connection(self.metadata_db) as conn:
            cursor = conn.execute(
                "SELECT doc_id, title, file_path, format, chunk_count, metadata, created_at, updated_at FROM documents WHERE doc_id = ?",
                (doc_id,)
            )
            row = cursor.fetchone()

            if not row:
                return None

            doc_data = dict(row)

        # è½½å…¥æ–‡æ¡£å—
        chunks = await self._load_chunks(doc_id)

        # è§£æå…ƒæ•°æ®
        metadata = doc_data.get('metadata', '{}')
        if isinstance(metadata, str):
            metadata = json.loads(metadata)

        doc = Document(
            doc_id=doc_data['doc_id'],
            title=doc_data['title'],
            content='',  # å®Œæ•´å†…å®¹ä¸å­˜å‚¨åœ¨å…ƒæ•°æ®ä¸­ï¼Œéœ€è¦ä»å—é‡ç»„
            format=doc_data['format'],
            file_path=doc_data['file_path'],
            metadata=metadata,
            chunks=chunks,
            chunk_count=doc_data.get('chunk_count', len(chunks)),
            indexed_at=doc_data.get('updated_at')  # ä½¿ç”¨ updated_at ä½œä¸ºç´¢å¼•æ—¶é—´
        )

        # ä»å—é‡å»ºå†…å®¹
        if chunks:
            doc.content = '\n\n'.join([chunk.content for chunk in chunks])

        # ç¼“å­˜æ–‡æ¡£
        if self.cache:
            self.cache.put(doc)

        return doc

    async def _load_chunks(self, doc_id: str) -> List[DocumentChunk]:
        """è½½å…¥æ–‡æ¡£å—"""
        if not self.vectors_db.exists():
            return []

        chunks = []
        with self._get_db_connection(self.vectors_db) as conn:
            # æ ¹æ® actual schema: document_vectors è¡¨
            cursor = conn.execute(
                "SELECT id, doc_id, chunk_index, content, keywords, metadata FROM document_vectors WHERE doc_id = ? ORDER BY chunk_index",
                (doc_id,)
            )
            rows = cursor.fetchall()

            for row in rows:
                chunk_data = dict(row)
                # è§£æå…ƒæ•°æ®
                metadata = chunk_data.get('metadata', '{}')
                if isinstance(metadata, str):
                    metadata = json.loads(metadata)

                chunks.append(DocumentChunk(
                    chunk_id=chunk_data['id'],
                    doc_id=chunk_data['doc_id'],
                    content=chunk_data['content'],
                    metadata=metadata,
                    embedding=None  # ç®€åŒ–ç‰ˆæœ¬ä¸ä½¿ç”¨ embedding
                ))

        return chunks

    async def load_batch(self, batch: int = 0) -> List[Document]:
        """è½½å…¥ä¸€æ‰¹æ–‡æ¡£"""
        docs_data = await self.list_documents(batch)

        if not docs_data:
            return []

        print(f"ğŸ“¦ è½½å…¥æ‰¹æ¬¡ {batch} ({len(docs_data)} ä¸ªæ–‡æ¡£)")

        # å¹¶å‘è½½å…¥æ–‡æ¡£
        tasks = [self.load_document(doc['doc_id']) for doc in docs_data]
        results = await asyncio.gather(*tasks)

        # è¿‡æ»¤ None å€¼
        documents = [doc for doc in results if doc is not None]

        return documents

    async def iterate_all_documents(self, callback: Callable[[Document], None]):
        """è¿­ä»£æ‰€æœ‰æ–‡æ¡£ï¼ˆåˆ†æ‰¹è½½å…¥ï¼‰"""
        batch = 0
        total_docs = 0

        while True:
            docs = await self.load_batch(batch)

            if not docs:
                break

            for doc in docs:
                await callback(doc)
                total_docs += 1

            batch += 1

            print(f"   å·²å¤„ç† {total_docs} ä¸ªæ–‡æ¡£")

    async def search_documents(self, query: str, limit: int = 10) -> List[Document]:
        """æœç´¢æ–‡æ¡£ï¼ˆé€šè¿‡å…³é”®è¯åŒ¹é…ï¼‰"""
        if not self.vectors_db.exists():
            return []

        # å…ˆä» document_vectors ä¸­æœç´¢åŒ¹é…çš„å—
        with self._get_db_connection(self.vectors_db) as conn:
            cursor = conn.execute(
                """
                SELECT DISTINCT doc_id
                FROM document_vectors
                WHERE content LIKE ? OR keywords LIKE ?
                LIMIT ?
                """,
                (f"%{query}%", f"%{query}%", limit * 2)  # è·å–æ›´å¤šå€™é€‰
            )
            rows = cursor.fetchall()

        if not rows:
            return []

        # è·å–å”¯ä¸€çš„ doc_id åˆ—è¡¨
        doc_ids = list(set([row['doc_id'] for row in rows]))[:limit]

        # è½½å…¥å®Œæ•´æ–‡æ¡£
        tasks = [self.load_document(doc_id) for doc_id in doc_ids]
        results = await asyncio.gather(*tasks)

        return [doc for doc in results if doc is not None]

    async def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_documents': 0,
            'total_chunks': 0,
            'by_category': {},
            'by_format': {},
            'oldest_doc': None,
            'newest_doc': None,
            'cache_size': self.cache.size() if self.cache else 0
        }

        if not self.metadata_db.exists():
            return stats

        with self._get_db_connection(self.metadata_db) as conn:
            # æ€»æ–‡æ¡£æ•°
            cursor = conn.execute("SELECT COUNT(*) as count FROM documents")
            stats['total_documents'] = cursor.fetchone()['count']

            # æŒ‰åˆ†ç±»ç»Ÿè®¡ï¼ˆä» metadata JSON ä¸­æå–ï¼‰
            cursor = conn.execute("""
                SELECT json_extract(metadata, '$.category') as category, COUNT(*) as count
                FROM documents
                GROUP BY category
            """)
            for row in cursor.fetchall():
                stats['by_category'][row['category'] or 'general'] = row['count']

            # æŒ‰æ ¼å¼ç»Ÿè®¡
            cursor = conn.execute("""
                SELECT format, COUNT(*) as count
                FROM documents
                GROUP BY format
            """)
            for row in cursor.fetchall():
                stats['by_format'][row['format']] = row['count']

            # æœ€æ—©å’Œæœ€æ–°çš„æ–‡æ¡£ï¼ˆä½¿ç”¨ created_at å’Œ updated_atï¼‰
            cursor = conn.execute("SELECT MIN(created_at) as oldest, MAX(updated_at) as newest FROM documents")
            row = cursor.fetchone()
            stats['oldest_doc'] = row['oldest']
            stats['newest_doc'] = row['newest']

        # æ€»å—æ•°ï¼ˆä» document_vectors è¡¨ï¼‰
        if self.vectors_db.exists():
            with self._get_db_connection(self.vectors_db) as conn:
                cursor = conn.execute("SELECT COUNT(*) as count FROM document_vectors")
                stats['total_chunks'] = cursor.fetchone()['count']

        return stats

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache:
            self.cache.clear()
            print("âœ… ç¼“å­˜å·²æ¸…ç©º")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ‰¹é‡æ–‡æ¡£è½½å…¥å™¨")
    parser.add_argument('--batch-size', type=int, default=20, help='æ¯æ‰¹è½½å…¥çš„æ–‡æ¡£æ•°é‡')
    parser.add_argument('--batch', type=int, default=0, help='è¦è½½å…¥çš„æ‰¹æ¬¡å·')
    parser.add_argument('--doc-id', type=str, help='è½½å…¥æŒ‡å®šæ–‡æ¡£ID')
    parser.add_argument('--search', type=str, help='æœç´¢æ–‡æ¡£')
    parser.add_argument('--category', type=str, help='æŒ‰åˆ†ç±»è¿‡æ»¤')
    parser.add_argument('--source', type=str, help='æŒ‰æ¥æºè¿‡æ»¤')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç©ºç¼“å­˜')

    args = parser.parse_args()

    try:
        # åˆ›å»ºé…ç½®
        config = LoadConfig(
            batch_size=args.batch_size,
            enable_cache=not args.no_cache,
            filter_category=args.category,
            filter_source=args.source
        )

        loader = BatchDocumentLoader(config)

        # æ¸…ç©ºç¼“å­˜
        if args.clear_cache:
            loader.clear_cache()
            return

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if args.stats:
            stats = await loader.get_statistics()
            print("\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
            print(f"   æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
            print(f"   æ€»å—æ•°: {stats['total_chunks']}")
            print(f"   ç¼“å­˜å¤§å°: {stats['cache_size']}")
            print(f"\n   æŒ‰åˆ†ç±»:")
            for category, count in sorted(stats['by_category'].items()):
                print(f"      {category}: {count}")
            print(f"\n   æŒ‰æ ¼å¼:")
            for format, count in sorted(stats['by_format'].items()):
                print(f"      {format}: {count}")
            print(f"\n   æ—¶é—´èŒƒå›´:")
            print(f"      æœ€æ—©: {stats['oldest_doc']}")
            print(f"      æœ€æ–°: {stats['newest_doc']}")
            return

        # è½½å…¥æŒ‡å®šæ–‡æ¡£
        if args.doc_id:
            doc = await loader.load_document(args.doc_id)
            if doc:
                print(f"\nğŸ“„ æ–‡æ¡£: {doc.title}")
                print(f"   ID: {doc.doc_id}")
                print(f"   æ ¼å¼: {doc.format}")
                print(f"   å—æ•°: {doc.chunk_count}")
                print(f"   å†…å®¹é•¿åº¦: {len(doc.content)} å­—ç¬¦")
            else:
                print(f"âŒ æœªæ‰¾åˆ°æ–‡æ¡£: {args.doc_id}")
            return

        # æœç´¢æ–‡æ¡£
        if args.search:
            docs = await loader.search_documents(args.search)
            print(f"\nğŸ” æœç´¢ç»“æœ: '{args.search}' ({len(docs)} ä¸ªæ–‡æ¡£)")
            for doc in docs:
                print(f"\n   ğŸ“„ {doc.title}")
                print(f"      ID: {doc.doc_id} | å—æ•°: {doc.chunk_count}")
            return

        # è½½å…¥æ‰¹æ¬¡
        docs = await loader.load_batch(args.batch)
        print(f"\nğŸ“¦ æ‰¹æ¬¡ {args.batch} è½½å…¥å®Œæˆ ({len(docs)} ä¸ªæ–‡æ¡£)")
        for doc in docs[:5]:
            print(f"   - ğŸ“„ {doc.title} ({doc.chunk_count} å—)")
        if len(docs) > 5:
            print(f"   ... è¿˜æœ‰ {len(docs) - 5} ä¸ªæ–‡æ¡£")

    except Exception as e:
        print(f"âŒ è½½å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
