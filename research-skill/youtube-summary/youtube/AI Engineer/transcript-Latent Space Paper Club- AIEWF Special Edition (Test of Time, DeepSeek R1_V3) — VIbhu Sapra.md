# Latent Space Paper Club- AIEWF Special Edition (Test of Time, DeepSeek R1_V3) â€” VIbhu Sapra

**Video URL:** https://www.youtube.com/watch?v=9k3xPh-40mo

---

## Full Transcript

### [00:00 - 01:00]

**[00:16]** Okay, so Paper Club year in review.

**[00:16]** Okay, so Paper Club year in review. We've gone over a year, like a year and

**[00:18]** We've gone over a year, like a year and

**[00:18]** We've gone over a year, like a year and a half, no missed weeks. We've always

**[00:21]** a half, no missed weeks. We've always

**[00:21]** a half, no missed weeks. We've always done Paper Club and it's pretty

**[00:22]** done Paper Club and it's pretty

**[00:22]** done Paper Club and it's pretty interesting. You know, I don't think any

**[00:24]** interesting. You know, I don't think any

**[00:24]** interesting. You know, I don't think any of us expected it to get this far, but

**[00:26]** of us expected it to get this far, but

**[00:26]** of us expected it to get this far, but every week for the past year and a half,

**[00:29]** every week for the past year and a half,

**[00:29]** every week for the past year and a half, we have always done a paper club

**[00:30]** we have always done a paper club

**[00:30]** we have always done a paper club Wednesday at noon. We've had a bunch of

**[00:32]** Wednesday at noon. We've had a bunch of

**[00:32]** Wednesday at noon. We've had a bunch of authors come share their work. People

**[00:34]** authors come share their work. People

**[00:34]** authors come share their work. People from Nvidia, Meta, uh, Alen Aai, Amazon

**[00:37]** from Nvidia, Meta, uh, Alen Aai, Amazon

**[00:37]** from Nvidia, Meta, uh, Alen Aai, Amazon together, writer, bunch of authors come,

**[00:39]** together, writer, bunch of authors come,

**[00:39]** together, writer, bunch of authors come, we get direct one-hour sessions with

**[00:41]** we get direct one-hour sessions with

**[00:41]** we get direct one-hour sessions with them. They share their work. We get nice

**[00:43]** them. They share their work. We get nice

**[00:43]** them. They share their work. We get nice feedback. And, you know, some of these

**[00:45]** feedback. And, you know, some of these

**[00:45]** feedback. And, you know, some of these have started to do pretty well. On

**[00:46]** have started to do pretty well. On

**[00:46]** have started to do pretty well. On average, we get about 100 people in here

**[00:48]** average, we get about 100 people in here

**[00:48]** average, we get about 100 people in here every every session. And just for you

**[00:51]** every every session. And just for you

**[00:51]** every every session. And just for you know um for for information that's like

**[00:54]** know um for for information that's like

**[00:54]** know um for for information that's like on a Wednesday workday at noon we have

**[00:57]** on a Wednesday workday at noon we have

**[00:57]** on a Wednesday workday at noon we have 100 people join to discuss a random

**[00:59]** 100 people join to discuss a random

**[00:59]** 100 people join to discuss a random paper. Um and some of the big ones


### [01:00 - 02:00]

**[01:02]** paper. Um and some of the big ones

**[01:02]** paper. Um and some of the big ones DeepSeek V3 had 300 live people sitting

**[01:05]** DeepSeek V3 had 300 live people sitting

**[01:05]** DeepSeek V3 had 300 live people sitting listening to me yap about uh Deepseek of

**[01:08]** listening to me yap about uh Deepseek of

**[01:08]** listening to me yap about uh Deepseek of course all the other speakers do great.

**[01:10]** course all the other speakers do great.

**[01:10]** course all the other speakers do great. This is all built on volunteers right

**[01:12]** This is all built on volunteers right

**[01:12]** This is all built on volunteers right and along the way we made many many

**[01:14]** and along the way we made many many

**[01:14]** and along the way we made many many friends and yeah paper club went much

**[01:16]** friends and yeah paper club went much

**[01:16]** friends and yeah paper club went much further than we uh expected. Now the the

**[01:19]** further than we uh expected. Now the the

**[01:19]** further than we uh expected. Now the the launch, you know, we have to ship

**[01:20]** launch, you know, we have to ship

**[01:20]** launch, you know, we have to ship something. We basically have to launch

**[01:22]** something. We basically have to launch

**[01:22]** something. We basically have to launch at World's Fair. So we're launching our

**[01:25]** at World's Fair. So we're launching our

**[01:25]** at World's Fair. So we're launching our test of time paper club. This is going

**[01:26]** test of time paper club. This is going

**[01:26]** test of time paper club. This is going to be a V2 second paper club. So the one

**[01:30]** to be a V2 second paper club. So the one

**[01:30]** to be a V2 second paper club. So the one that we do now or Wednesday at noon,

**[01:31]** that we do now or Wednesday at noon,

**[01:31]** that we do now or Wednesday at noon, this is still sticking around. It will

**[01:33]** this is still sticking around. It will

**[01:33]** this is still sticking around. It will still be the same thing. Every week

**[01:35]** still be the same thing. Every week

**[01:35]** still be the same thing. Every week we'll kind of have a paper, something

**[01:36]** we'll kind of have a paper, something

**[01:36]** we'll kind of have a paper, something that's trending, cover it, have an

**[01:38]** that's trending, cover it, have an

**[01:38]** that's trending, cover it, have an author, have our Q&A session, you know,

**[01:40]** author, have our Q&A session, you know,

**[01:40]** author, have our Q&A session, you know, 30 minutes of paper presentation,

**[01:41]** 30 minutes of paper presentation,

**[01:42]** 30 minutes of paper presentation, whether that's highlights or that's

**[01:43]** whether that's highlights or that's

**[01:43]** whether that's highlights or that's slides, and then we'll continue that

**[01:44]** slides, and then we'll continue that

**[01:44]** slides, and then we'll continue that with some discussion. But Tesla time

**[01:47]** with some discussion. But Tesla time

**[01:47]** with some discussion. But Tesla time paper club is going to be a little bit

**[01:49]** paper club is going to be a little bit

**[01:49]** paper club is going to be a little bit different, you know. So it's more

**[01:50]** different, you know. So it's more

**[01:50]** different, you know. So it's more curriculum based. Basically, we take

**[01:52]** curriculum based. Basically, we take

**[01:52]** curriculum based. Basically, we take this idea of what's everything that you

**[01:54]** this idea of what's everything that you

**[01:54]** this idea of what's everything that you would need to know to be a good AI

**[01:55]** would need to know to be a good AI

**[01:55]** would need to know to be a good AI engineer. What are the themes? What are

**[01:58]** engineer. What are the themes? What are

**[01:58]** engineer. What are the themes? What are the core papers that don't change? So

**[01:59]** the core papers that don't change? So

**[01:59]** the core papers that don't change? So stuff like, you know, what is attention?


### [02:00 - 03:00]

**[02:02]** stuff like, you know, what is attention?

**[02:02]** stuff like, you know, what is attention? How does uh sequential text generation

**[02:04]** How does uh sequential text generation

**[02:04]** How does uh sequential text generation work? So what's going on in GPT2? How do

**[02:06]** work? So what's going on in GPT2? How do

**[02:06]** work? So what's going on in GPT2? How do like optimizers work? What about like

**[02:08]** like optimizers work? What about like

**[02:08]** like optimizers work? What about like the key inference techniques, right? So

**[02:10]** the key inference techniques, right? So

**[02:10]** the key inference techniques, right? So stuff like speculative decoding, flash

**[02:12]** stuff like speculative decoding, flash

**[02:12]** stuff like speculative decoding, flash attention, uh stable diffusion, whisper,

**[02:15]** attention, uh stable diffusion, whisper,

**[02:15]** attention, uh stable diffusion, whisper, the key papers that you know are the

**[02:17]** the key papers that you know are the

**[02:17]** the key papers that you know are the foundation of what's being built. We're

**[02:19]** foundation of what's being built. We're

**[02:19]** foundation of what's being built. We're going to kind of group those together

**[02:20]** going to kind of group those together

**[02:20]** going to kind of group those together and session by session we'll go over

**[02:22]** and session by session we'll go over

**[02:22]** and session by session we'll go over them. So we're going to kick off in July

**[02:24]** them. So we're going to kick off in July

**[02:24]** them. So we're going to kick off in July and run till December. That leaves us

**[02:27]** and run till December. That leaves us

**[02:27]** and run till December. That leaves us six months. Six months is about four

**[02:29]** six months. Six months is about four

**[02:29]** six months. Six months is about four weeks a month. You know, 24 weeks. Every

**[02:31]** weeks a month. You know, 24 weeks. Every

**[02:31]** weeks a month. You know, 24 weeks. Every week we plan to go through two to four

**[02:33]** week we plan to go through two to four

**[02:33]** week we plan to go through two to four pre-presented papers. So you kind of

**[02:35]** pre-presented papers. So you kind of

**[02:35]** pre-presented papers. So you kind of have a bit more structure to it. There

**[02:37]** have a bit more structure to it. There

**[02:37]** have a bit more structure to it. There will be presentations but you know in

**[02:39]** will be presentations but you know in

**[02:39]** will be presentations but you know in six months we can get through like 50 to

**[02:41]** six months we can get through like 50 to

**[02:41]** six months we can get through like 50 to 100 papers and we can cover the core

**[02:44]** 100 papers and we can cover the core

**[02:44]** 100 papers and we can cover the core concepts and every week will be kind of

**[02:46]** concepts and every week will be kind of

**[02:46]** concepts and every week will be kind of different right so like one week we

**[02:48]** different right so like one week we

**[02:48]** different right so like one week we might be talking about whisper and

**[02:49]** might be talking about whisper and

**[02:49]** might be talking about whisper and you're interested right in one week you

**[02:51]** you're interested right in one week you

**[02:51]** you're interested right in one week you can get the fundamentals of speech uh

**[02:53]** can get the fundamentals of speech uh

**[02:53]** can get the fundamentals of speech uh speech to text to speech all that stuff

**[02:55]** speech to text to speech all that stuff

**[02:55]** speech to text to speech all that stuff another day it might be something like

**[02:57]** another day it might be something like

**[02:57]** another day it might be something like image generation so you know we'll go

**[02:58]** image generation so you know we'll go

**[02:58]** image generation so you know we'll go over clip stable diffusion how does all


### [03:00 - 04:00]

**[03:00]** over clip stable diffusion how does all

**[03:00]** over clip stable diffusion how does all this stuff work extend it out to video

**[03:03]** this stuff work extend it out to video

**[03:03]** this stuff work extend it out to video basically we'll segment the core topics

**[03:05]** basically we'll segment the core topics

**[03:05]** basically we'll segment the core topics that we should need to

**[03:07]** that we should need to

**[03:07]** that we should need to and then we'll cover it every week. Uh

**[03:09]** and then we'll cover it every week. Uh

**[03:09]** and then we'll cover it every week. Uh the exciting announcement here is we're

**[03:11]** the exciting announcement here is we're

**[03:11]** the exciting announcement here is we're also for the first time having uh SF

**[03:13]** also for the first time having uh SF

**[03:13]** also for the first time having uh SF section. So we're going to do uh

**[03:16]** section. So we're going to do uh

**[03:16]** section. So we're going to do uh inperson SF paper clubs and a remote

**[03:19]** inperson SF paper clubs and a remote

**[03:19]** inperson SF paper clubs and a remote section. I am going to mute someone real

**[03:21]** section. I am going to mute someone real

**[03:22]** section. I am going to mute someone real quick.

**[03:23]** quick.

**[03:23]** quick. So we've got 40 of you. Someone needs to

**[03:26]** So we've got 40 of you. Someone needs to

**[03:26]** So we've got 40 of you. Someone needs to be muted.

**[03:28]** be muted.

**[03:28]** be muted. We've got crazy echo. Oh, crazy. I'll

**[03:30]** We've got crazy echo. Oh, crazy. I'll

**[03:30]** We've got crazy echo. Oh, crazy. I'll just mute myself. Easy. I'll mute my own

**[03:32]** just mute myself. Easy. I'll mute my own

**[03:32]** just mute myself. Easy. I'll mute my own laptop. Okay, continuing on. Um if if

**[03:34]** laptop. Okay, continuing on. Um if if

**[03:34]** laptop. Okay, continuing on. Um if if someone needs to interrupt just just

**[03:36]** someone needs to interrupt just just

**[03:36]** someone needs to interrupt just just start in zoom chat because I don't have

**[03:37]** start in zoom chat because I don't have

**[03:37]** start in zoom chat because I don't have my speakers going but yeah um we'll have

**[03:40]** my speakers going but yeah um we'll have

**[03:40]** my speakers going but yeah um we'll have an in-person session in San Francisco

**[03:42]** an in-person session in San Francisco

**[03:42]** an in-person session in San Francisco every week and of course we'll keep the

**[03:44]** every week and of course we'll keep the

**[03:44]** every week and of course we'll keep the remote thing you know so we'll keep it

**[03:45]** remote thing you know so we'll keep it

**[03:45]** remote thing you know so we'll keep it going and original paper club will still

**[03:48]** going and original paper club will still

**[03:48]** going and original paper club will still be its own you know every week something

**[03:49]** be its own you know every week something

**[03:49]** be its own you know every week something comes out we'll cover that and we won't

**[03:51]** comes out we'll cover that and we won't

**[03:51]** comes out we'll cover that and we won't really deviate too much on the schedule

**[03:53]** really deviate too much on the schedule

**[03:53]** really deviate too much on the schedule for test of time it's going to be

**[03:54]** for test of time it's going to be

**[03:54]** for test of time it's going to be foundational papers plus some blogs um

**[03:57]** foundational papers plus some blogs um

**[03:57]** foundational papers plus some blogs um so what are the topics you know this is

**[03:59]** so what are the topics you know this is

**[03:59]** so what are the topics you know this is still up for us to all decide so just


### [04:00 - 05:00]

**[04:02]** still up for us to all decide so just

**[04:02]** still up for us to all decide so just listing out some of the few ones here

**[04:04]** listing out some of the few ones here

**[04:04]** listing out some of the few ones here you know we have foundations of deep

**[04:06]** you know we have foundations of deep

**[04:06]** you know we have foundations of deep learning something like attention

**[04:08]** learning something like attention

**[04:08]** learning something like attention optimization relu gradient descent what

**[04:11]** optimization relu gradient descent what

**[04:11]** optimization relu gradient descent what is basic RL how do these things work so

**[04:13]** is basic RL how do these things work so

**[04:13]** is basic RL how do these things work so you know we'll have a section on

**[04:15]** you know we'll have a section on

**[04:15]** you know we'll have a section on foundations of deep learning another one

**[04:17]** foundations of deep learning another one

**[04:17]** foundations of deep learning another one LLM foundations right so prels we have

**[04:21]** LLM foundations right so prels we have

**[04:21]** LLM foundations right so prels we have like RNN's uh LSTMs birectional RNN's

**[04:26]** like RNN's uh LSTMs birectional RNN's

**[04:26]** like RNN's uh LSTMs birectional RNN's all this stuff then we have like the

**[04:28]** all this stuff then we have like the

**[04:28]** all this stuff then we have like the very very foundational models right so

**[04:29]** very very foundational models right so

**[04:29]** very very foundational models right so like BERT from Google GPT2 stuff like

**[04:32]** like BERT from Google GPT2 stuff like

**[04:32]** like BERT from Google GPT2 stuff like that We'll have a day of everything kind

**[04:34]** that We'll have a day of everything kind

**[04:34]** that We'll have a day of everything kind of pre-LLM and go over those. Then after

**[04:37]** of pre-LLM and go over those. Then after

**[04:37]** of pre-LLM and go over those. Then after that, we'll have, you know, the the

**[04:39]** that, we'll have, you know, the the

**[04:39]** that, we'll have, you know, the the actual like generative LLMs. I'm sure

**[04:41]** actual like generative LLMs. I'm sure

**[04:41]** actual like generative LLMs. I'm sure that's missing here, but you know, like

**[04:43]** that's missing here, but you know, like

**[04:43]** that's missing here, but you know, like Llama 3, DeepSeek, the the core LLMs

**[04:45]** Llama 3, DeepSeek, the the core LLMs

**[04:45]** Llama 3, DeepSeek, the the core LLMs that you would expect to hear about.

**[04:46]** that you would expect to hear about.

**[04:46]** that you would expect to hear about. We'll go over those. We'll have a day of

**[04:48]** We'll go over those. We'll have a day of

**[04:48]** We'll go over those. We'll have a day of pre-made postraining, right? So, what

**[04:50]** pre-made postraining, right? So, what

**[04:50]** pre-made postraining, right? So, what are the scaling law papers? What is

**[04:52]** are the scaling law papers? What is

**[04:52]** are the scaling law papers? What is chinchilla? How does distillation work?

**[04:54]** chinchilla? How does distillation work?

**[04:54]** chinchilla? How does distillation work? What are the kind of key papers that you

**[04:56]** What are the kind of key papers that you

**[04:56]** What are the kind of key papers that you would want to know for um training? And

**[04:59]** would want to know for um training? And

**[04:59]** would want to know for um training? And once again, this will be like a one to


### [05:00 - 06:00]

**[05:01]** once again, this will be like a one to

**[05:01]** once again, this will be like a one to two session. So in one week we'll go

**[05:03]** two session. So in one week we'll go

**[05:04]** two session. So in one week we'll go over three to four papers. We'll have

**[05:06]** over three to four papers. We'll have

**[05:06]** over three to four papers. We'll have someone present. So come prepared. We'll

**[05:08]** someone present. So come prepared. We'll

**[05:08]** someone present. So come prepared. We'll go over you know what are the

**[05:09]** go over you know what are the

**[05:09]** go over you know what are the fundamentals in scaling laws,

**[05:11]** fundamentals in scaling laws,

**[05:11]** fundamentals in scaling laws, distillation, what is chinchilla

**[05:12]** distillation, what is chinchilla

**[05:12]** distillation, what is chinchilla scaling, what is uh overtraining, what

**[05:15]** scaling, what is uh overtraining, what

**[05:15]** scaling, what is uh overtraining, what is llama scaling laws, what are small

**[05:17]** is llama scaling laws, what are small

**[05:17]** is llama scaling laws, what are small models scaling laws. So like the fi team

**[05:19]** models scaling laws. So like the fi team

**[05:19]** models scaling laws. So like the fi team really has you know for small language

**[05:21]** really has you know for small language

**[05:21]** really has you know for small language models, how should we do proper RL? What

**[05:23]** models, how should we do proper RL? What

**[05:23]** models, how should we do proper RL? What is post training for small models versus

**[05:25]** is post training for small models versus

**[05:25]** is post training for small models versus big models? Well, we'll have someone

**[05:26]** big models? Well, we'll have someone

**[05:26]** big models? Well, we'll have someone cover all these and that'll be a one to

**[05:28]** cover all these and that'll be a one to

**[05:28]** cover all these and that'll be a one to two week session. We'll have generative

**[05:30]** two week session. We'll have generative

**[05:30]** two week session. We'll have generative models. So you know clip sora segment

**[05:32]** models. So you know clip sora segment

**[05:32]** models. So you know clip sora segment anything diffusion some of the key agent

**[05:34]** anything diffusion some of the key agent

**[05:34]** anything diffusion some of the key agent papers fine-tuning we'll go over Laura Q

**[05:37]** papers fine-tuning we'll go over Laura Q

**[05:37]** papers fine-tuning we'll go over Laura Q Laura DPO RL GRPO uh voice we'll have

**[05:41]** Laura DPO RL GRPO uh voice we'll have

**[05:41]** Laura DPO RL GRPO uh voice we'll have whisper optimization stage so you know

**[05:44]** whisper optimization stage so you know

**[05:44]** whisper optimization stage so you know speculative decoding flash attention

**[05:46]** speculative decoding flash attention

**[05:46]** speculative decoding flash attention we'll have eval tracks Rexus Eugene Yan

**[05:48]** we'll have eval tracks Rexus Eugene Yan

**[05:48]** we'll have eval tracks Rexus Eugene Yan you know he hosts our original paper

**[05:50]** you know he hosts our original paper

**[05:50]** you know he hosts our original paper club he'll fill in good ones there much

**[05:52]** club he'll fill in good ones there much

**[05:52]** club he'll fill in good ones there much much more um so yeah you know these

**[05:55]** much more um so yeah you know these

**[05:55]** much more um so yeah you know these categories are still up for uh debate so

**[05:57]** categories are still up for uh debate so

**[05:57]** categories are still up for uh debate so I have a form here later fill it out on


### [06:00 - 07:00]

**[06:00]** I have a form here later fill it out on

**[06:00]** I have a form here later fill it out on stuff that you would want to add, any

**[06:02]** stuff that you would want to add, any

**[06:02]** stuff that you would want to add, any papers that you recommend, any topics.

**[06:05]** papers that you recommend, any topics.

**[06:05]** papers that you recommend, any topics. It's it's very straightforward, you

**[06:06]** It's it's very straightforward, you

**[06:06]** It's it's very straightforward, you know. Uh also join our discord. We have

**[06:08]** know. Uh also join our discord. We have

**[06:08]** know. Uh also join our discord. We have the paper club channel and discord. So

**[06:10]** the paper club channel and discord. So

**[06:10]** the paper club channel and discord. So join that, add in topics, anything if

**[06:13]** join that, add in topics, anything if

**[06:13]** join that, add in topics, anything if you want to cover, you know, if you want

**[06:15]** you want to cover, you know, if you want

**[06:15]** you want to cover, you know, if you want to volunteer to be a speaker. Over the

**[06:17]** to volunteer to be a speaker. Over the

**[06:17]** to volunteer to be a speaker. Over the next like week, I'll I'll flesh out a

**[06:19]** next like week, I'll I'll flesh out a

**[06:19]** next like week, I'll I'll flesh out a rough schedule and then from there,

**[06:21]** rough schedule and then from there,

**[06:21]** rough schedule and then from there, we'll we'll kind of take it, you know,

**[06:23]** we'll we'll kind of take it, you know,

**[06:23]** we'll we'll kind of take it, you know, start covering it. Make sure that every

**[06:24]** start covering it. Make sure that every

**[06:24]** start covering it. Make sure that every session we have speakers. We'll figure

**[06:26]** session we have speakers. We'll figure

**[06:26]** session we have speakers. We'll figure out logistics. So SF will have a venue.

**[06:29]** out logistics. So SF will have a venue.

**[06:29]** out logistics. So SF will have a venue. We have a few in mind. Remote, it'll be

**[06:31]** We have a few in mind. Remote, it'll be

**[06:31]** We have a few in mind. Remote, it'll be the same uh Zoom thing, but yeah, we

**[06:33]** the same uh Zoom thing, but yeah, we

**[06:33]** the same uh Zoom thing, but yeah, we want to we want to fragment this out,

**[06:35]** want to we want to fragment this out,

**[06:35]** want to we want to fragment this out, get people get people interested. We'll

**[06:37]** get people get people interested. We'll

**[06:37]** get people get people interested. We'll obviously still bring in speakers, you

**[06:39]** obviously still bring in speakers, you

**[06:39]** obviously still bring in speakers, you know, so some of these key authors, we

**[06:41]** know, so some of these key authors, we

**[06:41]** know, so some of these key authors, we know them. So, we'll we'll invite them

**[06:43]** know them. So, we'll we'll invite them

**[06:43]** know them. So, we'll we'll invite them to speak on these papers and it'll be

**[06:45]** to speak on these papers and it'll be

**[06:45]** to speak on these papers and it'll be good. We'll have discussion sessions.

**[06:46]** good. We'll have discussion sessions.

**[06:46]** good. We'll have discussion sessions. These these sessions might be a little

**[06:48]** These these sessions might be a little

**[06:48]** These these sessions might be a little bit more than an hour. Since we now have

**[06:50]** bit more than an hour. Since we now have

**[06:50]** bit more than an hour. Since we now have three to four papers, we still want to

**[06:52]** three to four papers, we still want to

**[06:52]** three to four papers, we still want to just go deep, right? We don't want to

**[06:53]** just go deep, right? We don't want to

**[06:53]** just go deep, right? We don't want to just like do a TLDDR of the paper. We

**[06:55]** just like do a TLDDR of the paper. We

**[06:55]** just like do a TLDDR of the paper. We want to do what are the found what are

**[06:57]** want to do what are the found what are

**[06:57]** want to do what are the found what are the fundamentals and then go a little

**[06:58]** the fundamentals and then go a little

**[06:58]** the fundamentals and then go a little bit deeper. So um stay in touch on


### [07:00 - 08:00]

**[07:01]** bit deeper. So um stay in touch on

**[07:01]** bit deeper. So um stay in touch on discord very very basic Google form but

**[07:04]** discord very very basic Google form but

**[07:04]** discord very very basic Google form but yeah now we'll have curriculum based

**[07:06]** yeah now we'll have curriculum based

**[07:06]** yeah now we'll have curriculum based stuff um you know like Flo here might

**[07:08]** stuff um you know like Flo here might

**[07:08]** stuff um you know like Flo here might talk about music generation uh Eugene

**[07:11]** talk about music generation uh Eugene

**[07:11]** talk about music generation uh Eugene will talk about states based stuff so

**[07:13]** will talk about states based stuff so

**[07:13]** will talk about states based stuff so key same members but just yeah second

**[07:15]** key same members but just yeah second

**[07:15]** key same members but just yeah second paper club. Now the the other part of

**[07:18]** paper club. Now the the other part of

**[07:18]** paper club. Now the the other part of this is you know if you kind of have an

**[07:20]** this is you know if you kind of have an

**[07:20]** this is you know if you kind of have an area of interest this will be like your

**[07:23]** area of interest this will be like your

**[07:23]** area of interest this will be like your go-to session of you know here are the

**[07:26]** go-to session of you know here are the

**[07:26]** go-to session of you know here are the five to 10 papers here's a presentation

**[07:28]** five to 10 papers here's a presentation

**[07:28]** five to 10 papers here's a presentation on them here's discussion and it will

**[07:30]** on them here's discussion and it will

**[07:30]** on them here's discussion and it will stay live on YouTube you know it's just

**[07:32]** stay live on YouTube you know it's just

**[07:32]** stay live on YouTube you know it's just you can kind of set in at any time and

**[07:34]** you can kind of set in at any time and

**[07:34]** you can kind of set in at any time and be caught up to date you'll you'll kind

**[07:36]** be caught up to date you'll you'll kind

**[07:36]** be caught up to date you'll you'll kind of go fundamentals to what you need to

**[07:38]** of go fundamentals to what you need to

**[07:38]** of go fundamentals to what you need to know for every session and of course

**[07:40]** know for every session and of course

**[07:40]** know for every session and of course we'll have the same lively discussion

**[07:42]** we'll have the same lively discussion

**[07:42]** we'll have the same lively discussion okay test of time paper club very very

**[07:44]** okay test of time paper club very very

**[07:44]** okay test of time paper club very very hype but Today is still paper club. We

**[07:47]** hype but Today is still paper club. We

**[07:47]** hype but Today is still paper club. We can't not do a paper. So, you know, this

**[07:49]** can't not do a paper. So, you know, this

**[07:49]** can't not do a paper. So, you know, this is just mochi pitcher cuz everyone at

**[07:51]** is just mochi pitcher cuz everyone at

**[07:51]** is just mochi pitcher cuz everyone at the conference is having dogs in their

**[07:53]** the conference is having dogs in their

**[07:53]** the conference is having dogs in their slides. So, I need them too. So, we

**[07:55]** slides. So, I need them too. So, we

**[07:55]** slides. So, I need them too. So, we can't just have a old paper club back

**[07:57]** can't just have a old paper club back

**[07:57]** can't just have a old paper club back here. We need our OG. So, today's paper


### [08:00 - 09:00]

**[08:00]** here. We need our OG. So, today's paper

**[08:00]** here. We need our OG. So, today's paper is going to be Deepseek. So, Deepseek

**[08:03]** is going to be Deepseek. So, Deepseek

**[08:03]** is going to be Deepseek. So, Deepseek was obviously a popular paper. I know a

**[08:05]** was obviously a popular paper. I know a

**[08:05]** was obviously a popular paper. I know a lot of people haven't had a chance to

**[08:07]** lot of people haven't had a chance to

**[08:08]** lot of people haven't had a chance to actually go through the paper and

**[08:10]** actually go through the paper and

**[08:10]** actually go through the paper and frankly I didn't have much time to prep.

**[08:11]** frankly I didn't have much time to prep.

**[08:12]** frankly I didn't have much time to prep. So, you know, I get to reuse my slides.

**[08:13]** So, you know, I get to reuse my slides.

**[08:13]** So, you know, I get to reuse my slides. I'm smart like that. uh this is also

**[08:15]** I'm smart like that. uh this is also

**[08:15]** I'm smart like that. uh this is also being recorded for the broader AI

**[08:17]** being recorded for the broader AI

**[08:17]** being recorded for the broader AI engineer conference workshops and

**[08:20]** engineer conference workshops and

**[08:20]** engineer conference workshops and speakers. So it's another point you know

**[08:22]** speakers. So it's another point you know

**[08:22]** speakers. So it's another point you know this is why we do paper club we get

**[08:24]** this is why we do paper club we get

**[08:24]** this is why we do paper club we get these discussions on papers out and then

**[08:27]** these discussions on papers out and then

**[08:27]** these discussions on papers out and then you know people can find them later like

**[08:29]** you know people can find them later like

**[08:29]** you know people can find them later like our original deepseek paper reading

**[08:31]** our original deepseek paper reading

**[08:31]** our original deepseek paper reading basically like you know that was just

**[08:33]** basically like you know that was just

**[08:33]** basically like you know that was just last minute let's highlight the paper

**[08:35]** last minute let's highlight the paper

**[08:35]** last minute let's highlight the paper make some basic slides have some

**[08:36]** make some basic slides have some

**[08:36]** make some basic slides have some discussion but guess what 300 people

**[08:38]** discussion but guess what 300 people

**[08:38]** discussion but guess what 300 people join live there's over a thousand views

**[08:39]** join live there's over a thousand views

**[08:39]** join live there's over a thousand views on a YouTube video of us just reading

**[08:42]** on a YouTube video of us just reading

**[08:42]** on a YouTube video of us just reading through a paper so it's a pretty key

**[08:44]** through a paper so it's a pretty key

**[08:44]** through a paper so it's a pretty key paper it's like one of the big open

**[08:46]** paper it's like one of the big open

**[08:46]** paper it's like one of the big open source papers uh models that kind of had

**[08:48]** source papers uh models that kind of had

**[08:48]** source papers uh models that kind of had a big transition so let's just go over

**[08:50]** a big transition so let's just go over

**[08:50]** a big transition so let's just go over it again. And of course there is new

**[08:52]** it again. And of course there is new

**[08:52]** it again. And of course there is new stuff. So as of this week um or last

**[08:55]** stuff. So as of this week um or last

**[08:55]** stuff. So as of this week um or last week we we have Deepseek R10528.

**[08:58]** week we we have Deepseek R10528.

**[08:58]** week we we have Deepseek R10528. So basically the May 28th update. Now um


### [09:00 - 10:00]

**[09:02]** So basically the May 28th update. Now um

**[09:02]** So basically the May 28th update. Now um you know there have been rumors that

**[09:03]** you know there have been rumors that

**[09:03]** you know there have been rumors that okay Deep Seek uh V2 is coming out. It's

**[09:06]** okay Deep Seek uh V2 is coming out. It's

**[09:06]** okay Deep Seek uh V2 is coming out. It's coming out and then people start

**[09:07]** coming out and then people start

**[09:07]** coming out and then people start launching stuff. Guess what? They didn't

**[09:08]** launching stuff. Guess what? They didn't

**[09:08]** launching stuff. Guess what? They didn't do it. They just did the same model.

**[09:10]** do it. They just did the same model.

**[09:10]** do it. They just did the same model. They called it a minor update but it's

**[09:12]** They called it a minor update but it's

**[09:12]** They called it a minor update but it's actually not that small of an update. So

**[09:14]** actually not that small of an update. So

**[09:14]** actually not that small of an update. So let's let's dig into what it basically

**[09:15]** let's let's dig into what it basically

**[09:15]** let's let's dig into what it basically is. Uh it's not V3 like revision 2. It's

**[09:18]** is. Uh it's not V3 like revision 2. It's

**[09:18]** is. Uh it's not V3 like revision 2. It's it's the same naming scheme, but it's

**[09:20]** it's the same naming scheme, but it's

**[09:20]** it's the same naming scheme, but it's it's significantly better actually. So,

**[09:23]** it's significantly better actually. So,

**[09:24]** it's significantly better actually. So, um, yep. Simon Willis in his keynote

**[09:26]** um, yep. Simon Willis in his keynote

**[09:26]** um, yep. Simon Willis in his keynote mentioned um how we don't have good

**[09:29]** mentioned um how we don't have good

**[09:29]** mentioned um how we don't have good naming for models. This is basically

**[09:31]** naming for models. This is basically

**[09:31]** naming for models. This is basically quite a step up, but they've kept the

**[09:33]** quite a step up, but they've kept the

**[09:33]** quite a step up, but they've kept the name. Um, so also plugging Simon's talk,

**[09:37]** name. Um, so also plugging Simon's talk,

**[09:37]** name. Um, so also plugging Simon's talk, uh, he gave a keynote for the past six

**[09:39]** uh, he gave a keynote for the past six

**[09:39]** uh, he gave a keynote for the past six months. What were the 30 models? He

**[09:41]** months. What were the 30 models? He

**[09:41]** months. What were the 30 models? He launched Pelican bench. It's his own

**[09:43]** launched Pelican bench. It's his own

**[09:43]** launched Pelican bench. It's his own benchmark of how he judges how good

**[09:44]** benchmark of how he judges how good

**[09:44]** benchmark of how he judges how good foundation models are. uh he needs to

**[09:47]** foundation models are. uh he needs to

**[09:47]** foundation models are. uh he needs to check the new Deepseek model. I don't

**[09:48]** check the new Deepseek model. I don't

**[09:48]** check the new Deepseek model. I don't think he did, but basically here's what

**[09:50]** think he did, but basically here's what

**[09:50]** think he did, but basically here's what they did. Um they did better post

**[09:53]** they did. Um they did better post

**[09:53]** they did. Um they did better post training on DeepSeek V3. And now guess

**[09:55]** training on DeepSeek V3. And now guess

**[09:55]** training on DeepSeek V3. And now guess what? It got better. So um some of this

**[09:58]** what? It got better. So um some of this

**[09:58]** what? It got better. So um some of this stuff they put out very little

**[09:59]** stuff they put out very little

**[09:59]** stuff they put out very little information, but when you dig in, you


### [10:00 - 11:00]

**[10:01]** information, but when you dig in, you

**[10:01]** information, but when you dig in, you see that one of the key things is it's

**[10:03]** see that one of the key things is it's

**[10:03]** see that one of the key things is it's much better at reasoning. So the AIM

**[10:05]** much better at reasoning. So the AIM

**[10:05]** much better at reasoning. So the AIM 2024 score went from 70% to 87.5%.

**[10:10]** 2024 score went from 70% to 87.5%.

**[10:10]** 2024 score went from 70% to 87.5%. That basically means, you know, this is

**[10:11]** That basically means, you know, this is

**[10:12]** That basically means, you know, this is a good reasoning benchmark from before.

**[10:14]** a good reasoning benchmark from before.

**[10:14]** a good reasoning benchmark from before. Now we have V3 matching the performance

**[10:17]** Now we have V3 matching the performance

**[10:17]** Now we have V3 matching the performance of 03 and 2.5 level um on math coding

**[10:20]** of 03 and 2.5 level um on math coding

**[10:20]** of 03 and 2.5 level um on math coding and reasoning which is quite a step up.

**[10:22]** and reasoning which is quite a step up.

**[10:22]** and reasoning which is quite a step up. You know it used to be like okay we have

**[10:25]** You know it used to be like okay we have

**[10:25]** You know it used to be like okay we have 01 level intelligence and open source.

**[10:28]** 01 level intelligence and open source.

**[10:28]** 01 level intelligence and open source. Yeah all we needed was a little bit more

**[10:30]** Yeah all we needed was a little bit more

**[10:30]** Yeah all we needed was a little bit more training and now we have 03 and 2.5

**[10:32]** training and now we have 03 and 2.5

**[10:32]** training and now we have 03 and 2.5 level intelligence. And this isn't even

**[10:34]** level intelligence. And this isn't even

**[10:34]** level intelligence. And this isn't even like a new model from them. This is just

**[10:36]** like a new model from them. This is just

**[10:36]** like a new model from them. This is just like let's do a little bit better post

**[10:38]** like let's do a little bit better post

**[10:38]** like let's do a little bit better post training. let's do a little bit more and

**[10:40]** training. let's do a little bit more and

**[10:40]** training. let's do a little bit more and we can get significant significant uh

**[10:43]** we can get significant significant uh

**[10:43]** we can get significant significant uh performance increases. Pretty wild,

**[10:45]** performance increases. Pretty wild,

**[10:45]** performance increases. Pretty wild, right? Like 18% um improvement on

**[10:49]** right? Like 18% um improvement on

**[10:49]** right? Like 18% um improvement on benchmarks and no one's really talking

**[10:50]** benchmarks and no one's really talking

**[10:50]** benchmarks and no one's really talking about this. Deepseek got a lot better.

**[10:52]** about this. Deepseek got a lot better.

**[10:52]** about this. Deepseek got a lot better. Uh one of the quotes that they say is

**[10:55]** Uh one of the quotes that they say is

**[10:55]** Uh one of the quotes that they say is originally the original DeepS V3 it

**[10:58]** originally the original DeepS V3 it

**[10:58]** originally the original DeepS V3 it would take 12,000 tokens to reason


### [11:00 - 12:00]

**[11:00]** would take 12,000 tokens to reason

**[11:00]** would take 12,000 tokens to reason through on average on the benchmark for

**[11:03]** through on average on the benchmark for

**[11:03]** through on average on the benchmark for an aime it would take 12,000 tokens of

**[11:05]** an aime it would take 12,000 tokens of

**[11:05]** an aime it would take 12,000 tokens of reasoning. They basically did more RL

**[11:08]** reasoning. They basically did more RL

**[11:08]** reasoning. They basically did more RL now it reasons more on on average it

**[11:10]** now it reasons more on on average it

**[11:10]** now it reasons more on on average it reasons for 25,000 tokens. So they got

**[11:13]** reasons for 25,000 tokens. So they got

**[11:13]** reasons for 25,000 tokens. So they got the model to do double the reasoning.

**[11:15]** the model to do double the reasoning.

**[11:15]** the model to do double the reasoning. One thing that we talk a lot about is

**[11:17]** One thing that we talk a lot about is

**[11:17]** One thing that we talk a lot about is scaling laws, right? So before we would

**[11:20]** scaling laws, right? So before we would

**[11:20]** scaling laws, right? So before we would do optimal scaling for base models. Now

**[11:23]** do optimal scaling for base models. Now

**[11:23]** do optimal scaling for base models. Now in Llama we kind of did overfit our

**[11:25]** in Llama we kind of did overfit our

**[11:25]** in Llama we kind of did overfit our training for inference time, right?

**[11:28]** training for inference time, right?

**[11:28]** training for inference time, right? Let's really overtrain so we can do fast

**[11:30]** Let's really overtrain so we can do fast

**[11:30]** Let's really overtrain so we can do fast basic inference. And then now in this

**[11:32]** basic inference. And then now in this

**[11:32]** basic inference. And then now in this world of test time compute where we do

**[11:34]** world of test time compute where we do

**[11:34]** world of test time compute where we do um we do more inference time compute we

**[11:38]** um we do more inference time compute we

**[11:38]** um we do more inference time compute we can also scale even more in that

**[11:39]** can also scale even more in that

**[11:39]** can also scale even more in that dimension. So original deepseek on

**[11:41]** dimension. So original deepseek on

**[11:42]** dimension. So original deepseek on average would reason for 12,000 tokens.

**[11:44]** average would reason for 12,000 tokens.

**[11:44]** average would reason for 12,000 tokens. The new model can double that. So in

**[11:47]** The new model can double that. So in

**[11:47]** The new model can double that. So in this domain we've doubled the amount of

**[11:48]** this domain we've doubled the amount of

**[11:48]** this domain we've doubled the amount of reasoning it could do and we have a lot

**[11:50]** reasoning it could do and we have a lot

**[11:50]** reasoning it could do and we have a lot of benchmarks to increase. You know 18%

**[11:52]** of benchmarks to increase. You know 18%

**[11:52]** of benchmarks to increase. You know 18% on a IME and it's a lot better at

**[11:54]** on a IME and it's a lot better at

**[11:54]** on a IME and it's a lot better at coding. So in this they intentionally

**[11:57]** coding. So in this they intentionally

**[11:57]** coding. So in this they intentionally wanted to do better JSON output function

**[11:59]** wanted to do better JSON output function


### [12:00 - 13:00]

**[12:00]** wanted to do better JSON output function calling and more reasoning and yeah they

**[12:02]** calling and more reasoning and yeah they

**[12:02]** calling and more reasoning and yeah they just dropped it like that. Here's kind

**[12:04]** just dropped it like that. Here's kind

**[12:04]** just dropped it like that. Here's kind of our you know benchmark chart. On most

**[12:06]** of our you know benchmark chart. On most

**[12:06]** of our you know benchmark chart. On most paper clubs we don't do benchmarks but

**[12:08]** paper clubs we don't do benchmarks but

**[12:08]** paper clubs we don't do benchmarks but yeah it's um you know it's actually kind

**[12:10]** yeah it's um you know it's actually kind

**[12:10]** yeah it's um you know it's actually kind of up there with 03 and Gemini 2.5. So

**[12:14]** of up there with 03 and Gemini 2.5. So

**[12:14]** of up there with 03 and Gemini 2.5. So uh you know the darkest color is our new

**[12:16]** uh you know the darkest color is our new

**[12:16]** uh you know the darkest color is our new revision of Deepseek R1. And yeah it's

**[12:18]** revision of Deepseek R1. And yeah it's

**[12:18]** revision of Deepseek R1. And yeah it's it's actually very good on most

**[12:20]** it's actually very good on most

**[12:20]** it's actually very good on most benchmarks. It's like significantly

**[12:23]** benchmarks. It's like significantly

**[12:23]** benchmarks. It's like significantly better than the original Deepseek R1.

**[12:25]** better than the original Deepseek R1.

**[12:25]** better than the original Deepseek R1. Um, and you can see this, you know,

**[12:27]** Um, and you can see this, you know,

**[12:27]** Um, and you can see this, you know, humanity human humanity's uh last exam,

**[12:29]** humanity human humanity's uh last exam,

**[12:29]** humanity human humanity's uh last exam, it basically went from not being able to

**[12:31]** it basically went from not being able to

**[12:31]** it basically went from not being able to do anything to, okay, now this thing can

**[12:33]** do anything to, okay, now this thing can

**[12:33]** do anything to, okay, now this thing can do well. What they did is now we can

**[12:34]** do well. What they did is now we can

**[12:34]** do well. What they did is now we can basically reason for twice as long.

**[12:36]** basically reason for twice as long.

**[12:36]** basically reason for twice as long. Okay, that's not the only drop. They

**[12:39]** Okay, that's not the only drop. They

**[12:39]** Okay, that's not the only drop. They also launched another distillation. This

**[12:42]** also launched another distillation. This

**[12:42]** also launched another distillation. This is kind of the interesting one. Not not

**[12:43]** is kind of the interesting one. Not not

**[12:43]** is kind of the interesting one. Not not many people really talked about this at

**[12:45]** many people really talked about this at

**[12:45]** many people really talked about this at all on Twitter, but um if we remember in

**[12:48]** all on Twitter, but um if we remember in

**[12:48]** all on Twitter, but um if we remember in the original Deepseek paper, what they

**[12:50]** the original Deepseek paper, what they

**[12:50]** the original Deepseek paper, what they did was outside of their um original

**[12:53]** did was outside of their um original

**[12:53]** did was outside of their um original DeepSeek model, they did three

**[12:55]** DeepSeek model, they did three

**[12:55]** DeepSeek model, they did three distillation models. They distilled the

**[12:57]** distillation models. They distilled the

**[12:57]** distillation models. They distilled the Quen series and the Llama series and

**[12:59]** Quen series and the Llama series and

**[12:59]** Quen series and the Llama series and they showed how distilling from the big


### [13:00 - 14:00]

**[13:01]** they showed how distilling from the big

**[13:01]** they showed how distilling from the big model, distilling on these reasoning

**[13:03]** model, distilling on these reasoning

**[13:03]** model, distilling on these reasoning traces, we can get really, really good

**[13:05]** traces, we can get really, really good

**[13:05]** traces, we can get really, really good performance on small models. Well, they

**[13:07]** performance on small models. Well, they

**[13:07]** performance on small models. Well, they did it again. They took Quen 38B and

**[13:10]** did it again. They took Quen 38B and

**[13:10]** did it again. They took Quen 38B and they did another distillation with their

**[13:12]** they did another distillation with their

**[13:12]** they did another distillation with their new reasoning model and they show that

**[13:14]** new reasoning model and they show that

**[13:14]** new reasoning model and they show that you know the new model this new basic

**[13:17]** you know the new model this new basic

**[13:17]** you know the new model this new basic distillation basically like kills the

**[13:19]** distillation basically like kills the

**[13:19]** distillation basically like kills the old one. So um basically when you look

**[13:22]** old one. So um basically when you look

**[13:22]** old one. So um basically when you look at their old distill versus their new

**[13:24]** at their old distill versus their new

**[13:24]** at their old distill versus their new distill they they get another 10%

**[13:26]** distill they they get another 10%

**[13:26]** distill they they get another 10% performance boost by just doing

**[13:28]** performance boost by just doing

**[13:28]** performance boost by just doing distillation from a better reasoning

**[13:30]** distillation from a better reasoning

**[13:30]** distillation from a better reasoning model. So in a few months time they were

**[13:32]** model. So in a few months time they were

**[13:32]** model. So in a few months time they were able to get uh DeepSeek to do more chain

**[13:36]** able to get uh DeepSeek to do more chain

**[13:36]** able to get uh DeepSeek to do more chain of thought more reasoning use that to

**[13:38]** of thought more reasoning use that to

**[13:38]** of thought more reasoning use that to distill down to an 8B and now we have an

**[13:41]** distill down to an 8B and now we have an

**[13:41]** distill down to an 8B and now we have an even better 8B. So very very interesting

**[13:44]** even better 8B. So very very interesting

**[13:44]** even better 8B. So very very interesting little note right not just do we get a

**[13:46]** little note right not just do we get a

**[13:46]** little note right not just do we get a 10% improvement from the last base model

**[13:48]** 10% improvement from the last base model

**[13:48]** 10% improvement from the last base model um their 8B distillation is actually

**[13:53]** um their 8B distillation is actually

**[13:53]** um their 8B distillation is actually matching performance of Quen 3's 235

**[13:56]** matching performance of Quen 3's 235

**[13:56]** matching performance of Quen 3's 235 billion 20B active thinking model.

**[13:58]** billion 20B active thinking model.

**[13:58]** billion 20B active thinking model. Horrible naming, I know, but let's take


### [14:00 - 15:00]

**[14:00]** Horrible naming, I know, but let's take

**[14:00]** Horrible naming, I know, but let's take a second to think about that, right? A a

**[14:03]** a second to think about that, right? A a

**[14:03]** a second to think about that, right? A a Quen 3 8B dense model. So, a small 8B

**[14:07]** Quen 3 8B dense model. So, a small 8B

**[14:07]** Quen 3 8B dense model. So, a small 8B model is matching the performance of the

**[14:10]** model is matching the performance of the

**[14:10]** model is matching the performance of the Quen 3 235B thinking model. And this is

**[14:14]** Quen 3 235B thinking model. And this is

**[14:14]** Quen 3 235B thinking model. And this is not a native thinking model, right? This

**[14:16]** not a native thinking model, right? This

**[14:16]** not a native thinking model, right? This is a base distillation model of an 8

**[14:19]** is a base distillation model of an 8

**[14:19]** is a base distillation model of an 8 billion parameter model just on

**[14:21]** billion parameter model just on

**[14:21]** billion parameter model just on distillation. So, logic matching

**[14:23]** distillation. So, logic matching

**[14:23]** distillation. So, logic matching distillation from a big model. were

**[14:25]** distillation from a big model. were

**[14:25]** distillation from a big model. were matching performance of their 235

**[14:27]** matching performance of their 235

**[14:27]** matching performance of their 235 billion thinking model. That's pretty

**[14:30]** billion thinking model. That's pretty

**[14:30]** billion thinking model. That's pretty wild. They didn't do this to the 32B,

**[14:33]** wild. They didn't do this to the 32B,

**[14:33]** wild. They didn't do this to the 32B, the 70B, but yeah, it's pretty crazy,

**[14:35]** the 70B, but yeah, it's pretty crazy,

**[14:35]** the 70B, but yeah, it's pretty crazy, right? Untalked about release that the

**[14:38]** right? Untalked about release that the

**[14:38]** right? Untalked about release that the new 8B does very very well. Um, how do

**[14:41]** new 8B does very very well. Um, how do

**[14:41]** new 8B does very very well. Um, how do we see this? We can see um, you know,

**[14:43]** we see this? We can see um, you know,

**[14:43]** we see this? We can see um, you know, the chain of thought improvements

**[14:45]** the chain of thought improvements

**[14:45]** the chain of thought improvements distill down really, really hard. This

**[14:47]** distill down really, really hard. This

**[14:47]** distill down really, really hard. This was one of the key findings in the

**[14:48]** was one of the key findings in the

**[14:48]** was one of the key findings in the original paper that um a better recipe

**[14:51]** original paper that um a better recipe

**[14:51]** original paper that um a better recipe for training small models is to distill

**[14:54]** for training small models is to distill

**[14:54]** for training small models is to distill down from big models and reasoning

**[14:56]** down from big models and reasoning

**[14:56]** down from big models and reasoning models make this even more efficient.

**[14:57]** models make this even more efficient.

**[14:58]** models make this even more efficient. And then this is just kind of their

**[14:59]** And then this is just kind of their

**[14:59]** And then this is just kind of their follow-up right we don't have a paper we


### [15:00 - 16:00]

**[15:00]** follow-up right we don't have a paper we

**[15:00]** follow-up right we don't have a paper we don't have too much on this but these

**[15:02]** don't have too much on this but these

**[15:02]** don't have too much on this but these are benchmarks that show it and of

**[15:03]** are benchmarks that show it and of

**[15:03]** are benchmarks that show it and of course model is open source open way

**[15:05]** course model is open source open way

**[15:05]** course model is open source open way everything. So those are kind of the um

**[15:10]** everything. So those are kind of the um

**[15:10]** everything. So those are kind of the um those are kind of the overviews. Uh

**[15:12]** those are kind of the overviews. Uh

**[15:12]** those are kind of the overviews. Uh let's see let's see now let's go over

**[15:15]** let's see let's see now let's go over

**[15:15]** let's see let's see now let's go over the actual um the original Deepseek

**[15:18]** the actual um the original Deepseek

**[15:18]** the actual um the original Deepseek paper. So that's kind of ending where we

**[15:21]** paper. So that's kind of ending where we

**[15:21]** paper. So that's kind of ending where we had um the new releases. So we have two

**[15:24]** had um the new releases. So we have two

**[15:24]** had um the new releases. So we have two two models. To recap, we have a new

**[15:26]** two models. To recap, we have a new

**[15:26]** two models. To recap, we have a new Deepseek version. So for May 2020 uh for

**[15:29]** Deepseek version. So for May 2020 uh for

**[15:29]** Deepseek version. So for May 2020 uh for May 28th, we have a new DeepSeek. It's

**[15:32]** May 28th, we have a new DeepSeek. It's

**[15:32]** May 28th, we have a new DeepSeek. It's now at on par with uh OpenAI's 03 and

**[15:35]** now at on par with uh OpenAI's 03 and

**[15:35]** now at on par with uh OpenAI's 03 and Gemini 2.5. Uh it's significantly better

**[15:38]** Gemini 2.5. Uh it's significantly better

**[15:38]** Gemini 2.5. Uh it's significantly better and it reasons for twice as long. We

**[15:41]** and it reasons for twice as long. We

**[15:41]** and it reasons for twice as long. We also took that model, we distilled it

**[15:43]** also took that model, we distilled it

**[15:43]** also took that model, we distilled it down to quen 3 8B and we have a much

**[15:45]** down to quen 3 8B and we have a much

**[15:45]** down to quen 3 8B and we have a much much better small 8B reasoning model. Um

**[15:48]** much better small 8B reasoning model. Um

**[15:48]** much better small 8B reasoning model. Um this shows that you know reasoning

**[15:51]** this shows that you know reasoning

**[15:51]** this shows that you know reasoning models distilled down very very

**[15:52]** models distilled down very very

**[15:52]** models distilled down very very efficiently and there's still a lot of

**[15:54]** efficiently and there's still a lot of

**[15:54]** efficiently and there's still a lot of juice to be squeezed out there. Now um

**[15:56]** juice to be squeezed out there. Now um

**[15:56]** juice to be squeezed out there. Now um from here for those that haven't seen

**[15:58]** from here for those that haven't seen

**[15:58]** from here for those that haven't seen it, we're going to take a two second


### [16:00 - 17:00]

**[16:00]** it, we're going to take a two second

**[16:00]** it, we're going to take a two second pause, see if there's any interesting

**[16:02]** pause, see if there's any interesting

**[16:02]** pause, see if there's any interesting questions. Um there's a hugging face

**[16:04]** questions. Um there's a hugging face

**[16:04]** questions. Um there's a hugging face link. If these benchmarks are public,

**[16:06]** link. If these benchmarks are public,

**[16:06]** link. If these benchmarks are public, won't models be trained to score better

**[16:08]** won't models be trained to score better

**[16:08]** won't models be trained to score better on these benchmarks? Yeah, benchmarks

**[16:09]** on these benchmarks? Yeah, benchmarks

**[16:10]** on these benchmarks? Yeah, benchmarks obviously have their uh cons, their

**[16:11]** obviously have their uh cons, their

**[16:11]** obviously have their uh cons, their flaws, but you know there's ways to see

**[16:14]** flaws, but you know there's ways to see

**[16:14]** flaws, but you know there's ways to see what models are overfit on them. How do

**[16:16]** what models are overfit on them. How do

**[16:16]** what models are overfit on them. How do they do in general performance? In

**[16:17]** they do in general performance? In

**[16:17]** they do in general performance? In general, the Deepseek models are

**[16:19]** general, the Deepseek models are

**[16:19]** general, the Deepseek models are actually doing very very well. So um

**[16:21]** actually doing very very well. So um

**[16:21]** actually doing very very well. So um from there, let's go into the original

**[16:23]** from there, let's go into the original

**[16:23]** from there, let's go into the original Deepseek model. So uh okay, Deepseek V3

**[16:27]** Deepseek model. So uh okay, Deepseek V3

**[16:27]** Deepseek model. So uh okay, Deepseek V3 hypest paper of the year. 300 people

**[16:29]** hypest paper of the year. 300 people

**[16:29]** hypest paper of the year. 300 people joined us live. Let's do a quick recap.

**[16:31]** joined us live. Let's do a quick recap.

**[16:31]** joined us live. Let's do a quick recap. This is basically me using my old slides

**[16:34]** This is basically me using my old slides

**[16:34]** This is basically me using my old slides because I can. But let's talk about what

**[16:36]** because I can. But let's talk about what

**[16:36]** because I can. But let's talk about what happened in Deep Deepseek. So we're

**[16:38]** happened in Deep Deepseek. So we're

**[16:38]** happened in Deep Deepseek. So we're going to kick off with a highle model

**[16:39]** going to kick off with a highle model

**[16:39]** going to kick off with a highle model overview. So what are the models they

**[16:41]** overview. So what are the models they

**[16:41]** overview. So what are the models they release? When they release this, it's

**[16:43]** release? When they release this, it's

**[16:43]** release? When they release this, it's not just DeepSync V3, right? They also

**[16:44]** not just DeepSync V3, right? They also

**[16:44]** not just DeepSync V3, right? They also have R1. Um what is inference time

**[16:47]** have R1. Um what is inference time

**[16:47]** have R1. Um what is inference time training training? What is test time

**[16:49]** training training? What is test time

**[16:49]** training training? What is test time compute? What makes reasoning models

**[16:51]** compute? What makes reasoning models

**[16:51]** compute? What makes reasoning models different? So if you guys don't

**[16:53]** different? So if you guys don't

**[16:53]** different? So if you guys don't remember, this was the first test time

**[16:55]** remember, this was the first test time

**[16:55]** remember, this was the first test time scaling open um you know open model,

**[16:58]** scaling open um you know open model,

**[16:58]** scaling open um you know open model, right? This was the first one that got

**[16:59]** right? This was the first one that got

**[16:59]** right? This was the first one that got good. OpenAI released 01. Uh cloud


### [17:00 - 18:00]

**[17:02]** good. OpenAI released 01. Uh cloud

**[17:02]** good. OpenAI released 01. Uh cloud released cloud thinking much later.

**[17:04]** released cloud thinking much later.

**[17:04]** released cloud thinking much later. Gemini thinking came much later. We

**[17:06]** Gemini thinking came much later. We

**[17:06]** Gemini thinking came much later. We didn't really understand what was

**[17:08]** didn't really understand what was

**[17:08]** didn't really understand what was happening. We thought there was like

**[17:09]** happening. We thought there was like

**[17:09]** happening. We thought there was like MCTS. Uh there was a lot of you know

**[17:11]** MCTS. Uh there was a lot of you know

**[17:11]** MCTS. Uh there was a lot of you know multicolor load research. What's going

**[17:13]** multicolor load research. What's going

**[17:13]** multicolor load research. What's going on? There was internal let's generate

**[17:15]** on? There was internal let's generate

**[17:15]** on? There was internal let's generate chain of thought. Let's train it to do

**[17:17]** chain of thought. Let's train it to do

**[17:17]** chain of thought. Let's train it to do chain of thought. But turns out uh

**[17:19]** chain of thought. But turns out uh

**[17:19]** chain of thought. But turns out uh DeepSeek comes out with this paper. They

**[17:21]** DeepSeek comes out with this paper. They

**[17:21]** DeepSeek comes out with this paper. They do a great model and they're like yo RL

**[17:23]** do a great model and they're like yo RL

**[17:23]** do a great model and they're like yo RL RL works. So two models were released.

**[17:27]** RL works. So two models were released.

**[17:27]** RL works. So two models were released. Uh DeepSeek R10 basically they they take

**[17:30]** Uh DeepSeek R10 basically they they take

**[17:30]** Uh DeepSeek R10 basically they they take a base model. They do a lot of gRPO RL.

**[17:33]** a base model. They do a lot of gRPO RL.

**[17:33]** a base model. They do a lot of gRPO RL. They have training templates, reward

**[17:34]** They have training templates, reward

**[17:34]** They have training templates, reward models. um they have this emergence

**[17:37]** models. um they have this emergence

**[17:37]** models. um they have this emergence capability reflection aha moments then

**[17:40]** capability reflection aha moments then

**[17:40]** capability reflection aha moments then they do um R1 which is basically a four-

**[17:43]** they do um R1 which is basically a four-

**[17:43]** they do um R1 which is basically a four- stage pipeline they have cold start

**[17:45]** stage pipeline they have cold start

**[17:45]** stage pipeline they have cold start reasoning RL stage rejection sampling

**[17:48]** reasoning RL stage rejection sampling

**[17:48]** reasoning RL stage rejection sampling and SFT and then you know of course the

**[17:50]** and SFT and then you know of course the

**[17:50]** and SFT and then you know of course the little um RL round two to get it to

**[17:53]** little um RL round two to get it to

**[17:53]** little um RL round two to get it to really really reason from there we'll

**[17:55]** really really reason from there we'll

**[17:55]** really really reason from there we'll talk about performance and eval of how

**[17:57]** talk about performance and eval of how

**[17:57]** talk about performance and eval of how does original R1 do um how is deepseek


### [18:00 - 19:00]

**[18:00]** does original R1 do um how is deepseek

**[18:00]** does original R1 do um how is deepseek then the original distillation models

**[18:02]** then the original distillation models

**[18:02]** then the original distillation models future work reproductions and whatnot

**[18:04]** future work reproductions and whatnot

**[18:04]** future work reproductions and whatnot what kind skip over the base deepseek um

**[18:08]** what kind skip over the base deepseek um

**[18:08]** what kind skip over the base deepseek um evals and performance because you know

**[18:10]** evals and performance because you know

**[18:10]** evals and performance because you know we already covered the new one but okay

**[18:12]** we already covered the new one but okay

**[18:12]** we already covered the new one but okay continuing on so highle overview for

**[18:15]** continuing on so highle overview for

**[18:15]** continuing on so highle overview for those that understand that uh don't

**[18:17]** those that understand that uh don't

**[18:18]** those that understand that uh don't really follow you know we keep hearing

**[18:20]** really follow you know we keep hearing

**[18:20]** really follow you know we keep hearing this term of test time scaling what is

**[18:22]** this term of test time scaling what is

**[18:22]** this term of test time scaling what is test time scaling what are thinking

**[18:23]** test time scaling what are thinking

**[18:24]** test time scaling what are thinking models and what does this mean so

**[18:26]** models and what does this mean so

**[18:26]** models and what does this mean so basically we got to a point where we

**[18:28]** basically we got to a point where we

**[18:28]** basically we got to a point where we started to overtrain our models we

**[18:31]** started to overtrain our models we

**[18:31]** started to overtrain our models we basically hit a scaling limit on how

**[18:33]** basically hit a scaling limit on how

**[18:33]** basically hit a scaling limit on how much we can train models Originally back

**[18:35]** much we can train models Originally back

**[18:35]** much we can train models Originally back in the day we used to do sort of these

**[18:37]** in the day we used to do sort of these

**[18:37]** in the day we used to do sort of these chinchilla scaling laws right we had a

**[18:40]** chinchilla scaling laws right we had a

**[18:40]** chinchilla scaling laws right we had a fixed compute budget we had a fixed

**[18:42]** fixed compute budget we had a fixed

**[18:42]** fixed compute budget we had a fixed amount of data set we would design a

**[18:45]** amount of data set we would design a

**[18:45]** amount of data set we would design a model around that so how many parameters

**[18:47]** model around that so how many parameters

**[18:47]** model around that so how many parameters should it be based on how much data we

**[18:49]** should it be based on how much data we

**[18:49]** should it be based on how much data we have let's fit a model to our data let's

**[18:52]** have let's fit a model to our data let's

**[18:52]** have let's fit a model to our data let's fit how many GPUs you GPUs we have and

**[18:54]** fit how many GPUs you GPUs we have and

**[18:54]** fit how many GPUs you GPUs we have and then let's train it to be kind of

**[18:55]** then let's train it to be kind of

**[18:55]** then let's train it to be kind of chinchilla optimal from there we kind of

**[18:59]** chinchilla optimal from there we kind of

**[18:59]** chinchilla optimal from there we kind of realized that okay this isn't this isn't


### [19:00 - 20:00]

**[19:01]** realized that okay this isn't this isn't

**[19:01]** realized that okay this isn't this isn't really what we want and we started

**[19:02]** really what we want and we started

**[19:02]** really what we want and we started really really scaling up our training.

**[19:04]** really really scaling up our training.

**[19:04]** really really scaling up our training. So we had stuff like you know mistrol

**[19:07]** So we had stuff like you know mistrol

**[19:07]** So we had stuff like you know mistrol llama 1, llama 2, llama 3. We started

**[19:09]** llama 1, llama 2, llama 3. We started

**[19:09]** llama 1, llama 2, llama 3. We started training these models from billions of

**[19:11]** training these models from billions of

**[19:11]** training these models from billions of tokens to trillions of tokens. So you

**[19:13]** tokens to trillions of tokens. So you

**[19:13]** tokens to trillions of tokens. So you know we had originally like a one be one

**[19:16]** know we had originally like a one be one

**[19:16]** know we had originally like a one be one trillion token. Then llama 3 was 15

**[19:18]** trillion token. Then llama 3 was 15

**[19:18]** trillion token. Then llama 3 was 15 trillion tokens. Now they're up to like

**[19:20]** trillion tokens. Now they're up to like

**[19:20]** trillion tokens. Now they're up to like 45 trillion tokens. So what we shifted

**[19:23]** 45 trillion tokens. So what we shifted

**[19:23]** 45 trillion tokens. So what we shifted was instead of training for uh you know

**[19:27]** was instead of training for uh you know

**[19:27]** was instead of training for uh you know model chinchilla optimal let's start

**[19:29]** model chinchilla optimal let's start

**[19:29]** model chinchilla optimal let's start training for this sort of inference

**[19:30]** training for this sort of inference

**[19:30]** training for this sort of inference optimal uh training regime. So instead

**[19:34]** optimal uh training regime. So instead

**[19:34]** optimal uh training regime. So instead of you know thinking about what we have

**[19:36]** of you know thinking about what we have

**[19:36]** of you know thinking about what we have now let's think about inference time as

**[19:38]** now let's think about inference time as

**[19:38]** now let's think about inference time as we scale this we want a model to be as

**[19:40]** we scale this we want a model to be as

**[19:40]** we scale this we want a model to be as densely packed as smart as possible. So

**[19:43]** densely packed as smart as possible. So

**[19:43]** densely packed as smart as possible. So llama is like okay basically if we

**[19:45]** llama is like okay basically if we

**[19:45]** llama is like okay basically if we continue training we don't really see

**[19:46]** continue training we don't really see

**[19:46]** continue training we don't really see degregation but the problem in this is

**[19:49]** degregation but the problem in this is

**[19:49]** degregation but the problem in this is it gets very very expensive right as you

**[19:51]** it gets very very expensive right as you

**[19:51]** it gets very very expensive right as you train more and more yeah it's very very

**[19:54]** train more and more yeah it's very very

**[19:54]** train more and more yeah it's very very heavily compute extensive and you know

**[19:56]** heavily compute extensive and you know

**[19:56]** heavily compute extensive and you know how many times can you scale this up

**[19:58]** how many times can you scale this up

**[19:58]** how many times can you scale this up like how much data do we have how much

**[19:59]** like how much data do we have how much

**[19:59]** like how much data do we have how much can we really fit in if we're at 45


### [20:00 - 21:00]

**[20:02]** can we really fit in if we're at 45

**[20:02]** can we really fit in if we're at 45 trillion parameters for like a 70B can

**[20:05]** trillion parameters for like a 70B can

**[20:05]** trillion parameters for like a 70B can we scale that up 10x again you know are

**[20:07]** we scale that up 10x again you know are

**[20:07]** we scale that up 10x again you know are we going to do 450 trillion parameters

**[20:09]** we going to do 450 trillion parameters

**[20:09]** we going to do 450 trillion parameters what if we want to 10x it again we're

**[20:12]** what if we want to 10x it again we're

**[20:12]** what if we want to 10x it again we're basically hitting the compute scale for

**[20:14]** basically hitting the compute scale for

**[20:14]** basically hitting the compute scale for training, right? We can't continually

**[20:16]** training, right? We can't continually

**[20:16]** training, right? We can't continually just keep scaling up our train runs

**[20:18]** just keep scaling up our train runs

**[20:18]** just keep scaling up our train runs because it's no longer like cost

**[20:20]** because it's no longer like cost

**[20:20]** because it's no longer like cost efficient, right? We're spending

**[20:21]** efficient, right? We're spending

**[20:21]** efficient, right? We're spending millions and millions and hundreds of

**[20:23]** millions and millions and hundreds of

**[20:23]** millions and millions and hundreds of millions of dollars on these train runs.

**[20:25]** millions of dollars on these train runs.

**[20:25]** millions of dollars on these train runs. You can only scale so much. So, a lot of

**[20:28]** You can only scale so much. So, a lot of

**[20:28]** You can only scale so much. So, a lot of hypothesis was, you know, there's going

**[20:30]** hypothesis was, you know, there's going

**[20:30]** hypothesis was, you know, there's going to be a sort of plateau and open models

**[20:33]** to be a sort of plateau and open models

**[20:33]** to be a sort of plateau and open models will start to catch up because, you

**[20:35]** will start to catch up because, you

**[20:35]** will start to catch up because, you know, we're already all scaling so much.

**[20:37]** know, we're already all scaling so much.

**[20:37]** know, we're already all scaling so much. But that's where you know we need to

**[20:39]** But that's where you know we need to

**[20:39]** But that's where you know we need to unlock another dimension which in this

**[20:41]** unlock another dimension which in this

**[20:41]** unlock another dimension which in this case was reasoning or test time

**[20:43]** case was reasoning or test time

**[20:43]** case was reasoning or test time training. So this is where we basically

**[20:45]** training. So this is where we basically

**[20:46]** training. So this is where we basically started to do um reasoning capabilities

**[20:48]** started to do um reasoning capabilities

**[20:48]** started to do um reasoning capabilities without any supervised data right. Um

**[20:51]** without any supervised data right. Um

**[20:51]** without any supervised data right. Um there were approaches to try to do you

**[20:53]** there were approaches to try to do you

**[20:53]** there were approaches to try to do you know let's generate a bunch of chain of

**[20:55]** know let's generate a bunch of chain of

**[20:55]** know let's generate a bunch of chain of thought reasoning style data. Let's do

**[20:57]** thought reasoning style data. Let's do

**[20:57]** thought reasoning style data. Let's do post training on it and yeah our models

**[20:59]** post training on it and yeah our models

**[20:59]** post training on it and yeah our models do a little better but this didn't


### [21:00 - 22:00]

**[21:01]** do a little better but this didn't

**[21:01]** do a little better but this didn't scale. What we needed was um we wanted

**[21:04]** scale. What we needed was um we wanted

**[21:04]** scale. What we needed was um we wanted to do pure RL. Can we do pure RL to do

**[21:07]** to do pure RL. Can we do pure RL to do

**[21:07]** to do pure RL. Can we do pure RL to do uh reasoning data? So this is a quote

**[21:10]** uh reasoning data? So this is a quote

**[21:10]** uh reasoning data? So this is a quote from the paper. Basically the Deepseek

**[21:13]** from the paper. Basically the Deepseek

**[21:13]** from the paper. Basically the Deepseek team says our goal is to explore the

**[21:15]** team says our goal is to explore the

**[21:15]** team says our goal is to explore the potential of LLMs to to develop

**[21:18]** potential of LLMs to to develop

**[21:18]** potential of LLMs to to develop reasoning capabilities without any

**[21:20]** reasoning capabilities without any

**[21:20]** reasoning capabilities without any supervised data f focusing on their

**[21:22]** supervised data f focusing on their

**[21:22]** supervised data f focusing on their self-evolution through a pure RL

**[21:24]** self-evolution through a pure RL

**[21:24]** self-evolution through a pure RL process. So what they're going to do is

**[21:26]** process. So what they're going to do is

**[21:26]** process. So what they're going to do is they're going to postrain the deepseek

**[21:29]** they're going to postrain the deepseek

**[21:29]** they're going to postrain the deepseek v3 base with gRPO which is you know

**[21:31]** v3 base with gRPO which is you know

**[21:31]** v3 base with gRPO which is you know basically pure RL and then as they do

**[21:34]** basically pure RL and then as they do

**[21:34]** basically pure RL and then as they do this they start to notice emergence of

**[21:36]** this they start to notice emergence of

**[21:36]** this they start to notice emergence of great reasoning reflection aha moments

**[21:39]** great reasoning reflection aha moments

**[21:39]** great reasoning reflection aha moments and they start to match 01. Now fast

**[21:42]** and they start to match 01. Now fast

**[21:42]** and they start to match 01. Now fast forward a few months as we can see

**[21:43]** forward a few months as we can see

**[21:43]** forward a few months as we can see today. Um they can basically continue

**[21:47]** today. Um they can basically continue

**[21:47]** today. Um they can basically continue this. They do a little bit more RL. They

**[21:49]** this. They do a little bit more RL. They

**[21:49]** this. They do a little bit more RL. They they do RL on longer traces. They can

**[21:52]** they do RL on longer traces. They can

**[21:52]** they do RL on longer traces. They can now get the model to not match 01 but

**[21:55]** now get the model to not match 01 but

**[21:55]** now get the model to not match 01 but match 03 and double its amount of

**[21:57]** match 03 and double its amount of

**[21:57]** match 03 and double its amount of reasoning tokens. Okay. Here's kind of

**[21:59]** reasoning tokens. Okay. Here's kind of

**[21:59]** reasoning tokens. Okay. Here's kind of the four-step approach to how they train


### [22:00 - 23:00]

**[22:01]** the four-step approach to how they train

**[22:01]** the four-step approach to how they train this R1 model. They start with this sort

**[22:03]** this R1 model. They start with this sort

**[22:03]** this R1 model. They start with this sort of cold start where you know they they

**[22:06]** of cold start where you know they they

**[22:06]** of cold start where you know they they jump start with some SFT then they do RL

**[22:08]** jump start with some SFT then they do RL

**[22:08]** jump start with some SFT then they do RL for reasoning. Then they have this key

**[22:10]** for reasoning. Then they have this key

**[22:10]** for reasoning. Then they have this key step of rejection sampling for

**[22:12]** step of rejection sampling for

**[22:12]** step of rejection sampling for generation purposes. You know rejection

**[22:13]** generation purposes. You know rejection

**[22:13]** generation purposes. You know rejection sampling is something we'll talk about

**[22:15]** sampling is something we'll talk about

**[22:15]** sampling is something we'll talk about later. Then once again they do the

**[22:16]** later. Then once again they do the

**[22:16]** later. Then once again they do the fourth stage of basic RL polishing.

**[22:19]** fourth stage of basic RL polishing.

**[22:19]** fourth stage of basic RL polishing. Okay. Um that's a very high level uh

**[22:23]** Okay. Um that's a very high level uh

**[22:23]** Okay. Um that's a very high level uh overview of what DeepS did to make RL.

**[22:25]** overview of what DeepS did to make RL.

**[22:25]** overview of what DeepS did to make RL. So to recap you know we needed to shift

**[22:28]** So to recap you know we needed to shift

**[22:28]** So to recap you know we needed to shift from next token predictors to scaling in

**[22:31]** from next token predictors to scaling in

**[22:31]** from next token predictors to scaling in another access. To do this we needed to

**[22:34]** another access. To do this we needed to

**[22:34]** another access. To do this we needed to scale on instead of spending the same

**[22:36]** scale on instead of spending the same

**[22:36]** scale on instead of spending the same compute for every token generated we

**[22:39]** compute for every token generated we

**[22:39]** compute for every token generated we want to dynamically generate we want

**[22:40]** want to dynamically generate we want

**[22:40]** want to dynamically generate we want dynamically spend more compute on

**[22:43]** dynamically spend more compute on

**[22:43]** dynamically spend more compute on different queries. So we train the

**[22:45]** different queries. So we train the

**[22:45]** different queries. So we train the models now with pure RL to reason

**[22:48]** models now with pure RL to reason

**[22:48]** models now with pure RL to reason through their questions. So instead, you

**[22:50]** through their questions. So instead, you

**[22:50]** through their questions. So instead, you know, they're now trained to do RL with

**[22:52]** know, they're now trained to do RL with

**[22:52]** know, they're now trained to do RL with verifiable outputs on a lot of code and

**[22:55]** verifiable outputs on a lot of code and

**[22:55]** verifiable outputs on a lot of code and math data that can be verified to be,

**[22:57]** math data that can be verified to be,

**[22:57]** math data that can be verified to be, you know, uh, whether it compiles,


### [23:00 - 24:00]

**[23:00]** you know, uh, whether it compiles,

**[23:00]** you know, uh, whether it compiles, whether it's factually correct, whether

**[23:02]** whether it's factually correct, whether

**[23:02]** whether it's factually correct, whether the math is logically, um, correct. And

**[23:04]** the math is logically, um, correct. And

**[23:04]** the math is logically, um, correct. And then now we can basically do native RL.

**[23:07]** then now we can basically do native RL.

**[23:07]** then now we can basically do native RL. Doing this, we notice emergence,

**[23:09]** Doing this, we notice emergence,

**[23:09]** Doing this, we notice emergence, reflection, aha moments. And now we

**[23:12]** reflection, aha moments. And now we

**[23:12]** reflection, aha moments. And now we basically have another domain in which

**[23:14]** basically have another domain in which

**[23:14]** basically have another domain in which to scale models. instead of scaling up a

**[23:16]** to scale models. instead of scaling up a

**[23:16]** to scale models. instead of scaling up a 10x order of magnitude of you know

**[23:18]** 10x order of magnitude of you know

**[23:18]** 10x order of magnitude of you know instead of 45 trillion tokens let's

**[23:20]** instead of 45 trillion tokens let's

**[23:20]** instead of 45 trillion tokens let's train on 450 trillion tokens and then

**[23:23]** train on 450 trillion tokens and then

**[23:24]** train on 450 trillion tokens and then you know scale that up and up we're kind

**[23:25]** you know scale that up and up we're kind

**[23:25]** you know scale that up and up we're kind of at the limit there we start with a

**[23:27]** of at the limit there we start with a

**[23:27]** of at the limit there we start with a really good base model that's a good

**[23:29]** really good base model that's a good

**[23:29]** really good base model that's a good general next token predictor we do RL

**[23:31]** general next token predictor we do RL

**[23:31]** general next token predictor we do RL and now we can scale in the reasoning

**[23:33]** and now we can scale in the reasoning

**[23:33]** and now we can scale in the reasoning domain so a few months ago they showed

**[23:35]** domain so a few months ago they showed

**[23:35]** domain so a few months ago they showed that you know they can get 01 level

**[23:37]** that you know they can get 01 level

**[23:37]** that you know they can get 01 level performance and then fast forward to now

**[23:39]** performance and then fast forward to now

**[23:39]** performance and then fast forward to now we have 03 level performance with just

**[23:41]** we have 03 level performance with just

**[23:41]** we have 03 level performance with just more RL tangentially this was kind of

**[23:44]** more RL tangentially this was kind of

**[23:44]** more RL tangentially this was kind of the paper to kick it all off, right? Um,

**[23:47]** the paper to kick it all off, right? Um,

**[23:47]** the paper to kick it all off, right? Um, Deepseek showed that two things. One,

**[23:50]** Deepseek showed that two things. One,

**[23:50]** Deepseek showed that two things. One, you can do RL from base models and we

**[23:52]** you can do RL from base models and we

**[23:52]** you can do RL from base models and we can get reasoning. Two, distillation

**[23:55]** can get reasoning. Two, distillation

**[23:55]** can get reasoning. Two, distillation really works and it's a much better

**[23:57]** really works and it's a much better

**[23:57]** really works and it's a much better approach to small models. Following

**[23:59]** approach to small models. Following

**[23:59]** approach to small models. Following this, we've now had a lot more papers.


### [24:00 - 25:00]

**[24:00]** this, we've now had a lot more papers.

**[24:00]** this, we've now had a lot more papers. So, uh, shout out to like the 54 models,

**[24:03]** So, uh, shout out to like the 54 models,

**[24:03]** So, uh, shout out to like the 54 models, right? 54 showed how effective RL can

**[24:06]** right? 54 showed how effective RL can

**[24:06]** right? 54 showed how effective RL can be. They took 54 mini and uh, they made

**[24:08]** be. They took 54 mini and uh, they made

**[24:08]** be. They took 54 mini and uh, they made it a reasoning variant. Basically, they

**[24:11]** it a reasoning variant. Basically, they

**[24:11]** it a reasoning variant. Basically, they took about 6,000 samples and in 6,000

**[24:14]** took about 6,000 samples and in 6,000

**[24:14]** took about 6,000 samples and in 6,000 samples of RL, they could take a small

**[24:16]** samples of RL, they could take a small

**[24:16]** samples of RL, they could take a small base model, a small base uh next token

**[24:19]** base model, a small base uh next token

**[24:19]** base model, a small base uh next token predictor and do really really good

**[24:20]** predictor and do really really good

**[24:20]** predictor and do really really good reasoning inference on it. So, this was

**[24:23]** reasoning inference on it. So, this was

**[24:23]** reasoning inference on it. So, this was the one to kick it off and now we have

**[24:25]** the one to kick it off and now we have

**[24:26]** the one to kick it off and now we have um you know the Quen models have done

**[24:27]** um you know the Quen models have done

**[24:27]** um you know the Quen models have done this. So, there's Quen thinking models

**[24:29]** this. So, there's Quen thinking models

**[24:29]** this. So, there's Quen thinking models or deepseek thinking models and then

**[24:30]** or deepseek thinking models and then

**[24:30]** or deepseek thinking models and then there's formulas like um the the the

**[24:34]** there's formulas like um the the the

**[24:34]** there's formulas like um the the the five models that show how to do this in

**[24:36]** five models that show how to do this in

**[24:36]** five models that show how to do this in small models. Okay, let's let's continue

**[24:38]** small models. Okay, let's let's continue

**[24:38]** small models. Okay, let's let's continue on from my high level overview. So, what

**[24:40]** on from my high level overview. So, what

**[24:40]** on from my high level overview. So, what did they release? They released two

**[24:42]** did they release? They released two

**[24:42]** did they release? They released two models early on. There's R10, which is a

**[24:45]** models early on. There's R10, which is a

**[24:45]** models early on. There's R10, which is a great reasoning model only trained on

**[24:47]** great reasoning model only trained on

**[24:47]** great reasoning model only trained on unraveled chain of thought with RL, but

**[24:50]** unraveled chain of thought with RL, but

**[24:50]** unraveled chain of thought with RL, but it's not a great general model. R10 is

**[24:52]** it's not a great general model. R10 is

**[24:52]** it's not a great general model. R10 is when you only do RL on chain of thought.

**[24:55]** when you only do RL on chain of thought.

**[24:55]** when you only do RL on chain of thought. You know, it doesn't it doesn't really

**[24:57]** You know, it doesn't it doesn't really

**[24:57]** You know, it doesn't it doesn't really emerge to general performance. So, R1 is


### [25:00 - 26:00]

**[25:01]** emerge to general performance. So, R1 is

**[25:01]** emerge to general performance. So, R1 is trained um as the second model. It uses

**[25:04]** trained um as the second model. It uses

**[25:04]** trained um as the second model. It uses outputs of R10 using this four stages of

**[25:07]** outputs of R10 using this four stages of

**[25:07]** outputs of R10 using this four stages of training and then you know now we start

**[25:09]** training and then you know now we start

**[25:09]** training and then you know now we start to do RL back on human task back on chat

**[25:13]** to do RL back on human task back on chat

**[25:13]** to do RL back on human task back on chat and we have a good good oral we have a

**[25:15]** and we have a good good oral we have a

**[25:15]** and we have a good good oral we have a good good um reasoning model. Second

**[25:17]** good good um reasoning model. Second

**[25:17]** good good um reasoning model. Second thing they release is their

**[25:19]** thing they release is their

**[25:19]** thing they release is their distillations right so they take these

**[25:22]** distillations right so they take these

**[25:22]** distillations right so they take these uh thinking traces and they take models

**[25:24]** uh thinking traces and they take models

**[25:24]** uh thinking traces and they take models in the same family and then they distill

**[25:26]** in the same family and then they distill

**[25:26]** in the same family and then they distill them down. So these are not natively

**[25:28]** them down. So these are not natively

**[25:28]** them down. So these are not natively trained with RL. They're um

**[25:30]** trained with RL. They're um

**[25:30]** trained with RL. They're um distillations from their base models in

**[25:32]** distillations from their base models in

**[25:32]** distillations from their base models in Quinn and Llama families. And then they

**[25:34]** Quinn and Llama families. And then they

**[25:34]** Quinn and Llama families. And then they show how this works really well. Okay.

**[25:36]** show how this works really well. Okay.

**[25:36]** show how this works really well. Okay. Now, of course, you know, it's 2025. We

**[25:39]** Now, of course, you know, it's 2025. We

**[25:39]** Now, of course, you know, it's 2025. We don't get real papers anymore. So they

**[25:41]** don't get real papers anymore. So they

**[25:41]** don't get real papers anymore. So they don't talk about data, how many tokens,

**[25:43]** don't talk about data, how many tokens,

**[25:43]** don't talk about data, how many tokens, where the data comes from, but you know,

**[25:46]** where the data comes from, but you know,

**[25:46]** where the data comes from, but you know, they still do share a lot about about

**[25:48]** they still do share a lot about about

**[25:48]** they still do share a lot about about how this stuff works. Um models of

**[25:50]** how this stuff works. Um models of

**[25:50]** how this stuff works. Um models of course fully open source MIT license, no

**[25:53]** course fully open source MIT license, no

**[25:53]** course fully open source MIT license, no training data, no code. They have a

**[25:55]** training data, no code. They have a

**[25:56]** training data, no code. They have a deepseek API which at the time you know

**[25:58]** deepseek API which at the time you know

**[25:58]** deepseek API which at the time you know it was much faster than anyone. It was


### [26:00 - 27:00]

**[26:00]** it was much faster than anyone. It was

**[26:00]** it was much faster than anyone. It was much cheaper than anyone. Turns out this

**[26:02]** much cheaper than anyone. Turns out this

**[26:02]** much cheaper than anyone. Turns out this was fake news. This API is very

**[26:04]** was fake news. This API is very

**[26:04]** was fake news. This API is very unreliable. It's barely ever up. But um

**[26:07]** unreliable. It's barely ever up. But um

**[26:07]** unreliable. It's barely ever up. But um you know now now a few months later

**[26:09]** you know now now a few months later

**[26:09]** you know now now a few months later after this is coming out after this has

**[26:11]** after this is coming out after this has

**[26:11]** after this is coming out after this has come out we can see stuff like from open

**[26:13]** come out we can see stuff like from open

**[26:13]** come out we can see stuff like from open router you know deepseek actually takes

**[26:15]** router you know deepseek actually takes

**[26:15]** router you know deepseek actually takes a significant chunk of the pie. They

**[26:17]** a significant chunk of the pie. They

**[26:17]** a significant chunk of the pie. They take about 10% of uh API usage that goes

**[26:19]** take about 10% of uh API usage that goes

**[26:19]** take about 10% of uh API usage that goes through them. So this model actually

**[26:21]** through them. So this model actually

**[26:21]** through them. So this model actually sparked a lot of adoption. It reopened

**[26:24]** sparked a lot of adoption. It reopened

**[26:24]** sparked a lot of adoption. It reopened up the race of you know, okay, we're not

**[26:26]** up the race of you know, okay, we're not

**[26:26]** up the race of you know, okay, we're not done scaling. Models are still getting a

**[26:27]** done scaling. Models are still getting a

**[26:27]** done scaling. Models are still getting a lot better and um yeah, you know, once

**[26:30]** lot better and um yeah, you know, once

**[26:30]** lot better and um yeah, you know, once again, it's been like a few months and

**[26:32]** again, it's been like a few months and

**[26:32]** again, it's been like a few months and as of like last week, we have another

**[26:34]** as of like last week, we have another

**[26:34]** as of like last week, we have another update to this and then the Quen team

**[26:36]** update to this and then the Quen team

**[26:36]** update to this and then the Quen team followed along. Uh what was fake news?

**[26:38]** followed along. Uh what was fake news?

**[26:38]** followed along. Uh what was fake news? Fake news was the Deep Seek API. When it

**[26:41]** Fake news was the Deep Seek API. When it

**[26:41]** Fake news was the Deep Seek API. When it launched, it was uh 10x faster and 10x

**[26:45]** launched, it was uh 10x faster and 10x

**[26:45]** launched, it was uh 10x faster and 10x cheaper than other inference providers,

**[26:47]** cheaper than other inference providers,

**[26:47]** cheaper than other inference providers, but turns out it was super unreliable.

**[26:49]** but turns out it was super unreliable.

**[26:49]** but turns out it was super unreliable. No one can make API keys. It was almost

**[26:51]** No one can make API keys. It was almost

**[26:51]** No one can make API keys. It was almost always down, but it's okay. You know, it

**[26:54]** always down, but it's okay. You know, it

**[26:54]** always down, but it's okay. You know, it exists if you still want to try it. It's

**[26:56]** exists if you still want to try it. It's

**[26:56]** exists if you still want to try it. It's a really good model. A lot of people use

**[26:58]** a really good model. A lot of people use

**[26:58]** a really good model. A lot of people use it. Okay. Um, let's dig deep into these


### [27:00 - 28:00]

**[27:01]** it. Okay. Um, let's dig deep into these

**[27:01]** it. Okay. Um, let's dig deep into these topics. Inference time scaling. So, what

**[27:03]** topics. Inference time scaling. So, what

**[27:03]** topics. Inference time scaling. So, what is inference time scaling? We have 01

**[27:05]** is inference time scaling? We have 01

**[27:05]** is inference time scaling? We have 01 versus GPT40. Now, there's 03, 04, 04

**[27:08]** versus GPT40. Now, there's 03, 04, 04

**[27:08]** versus GPT40. Now, there's 03, 04, 04 mini, all these things. But, um,

**[27:10]** mini, all these things. But, um,

**[27:10]** mini, all these things. But, um, basically what these do is you increase

**[27:13]** basically what these do is you increase

**[27:13]** basically what these do is you increase the chain of thought reasoning process.

**[27:15]** the chain of thought reasoning process.

**[27:15]** the chain of thought reasoning process. You allow models to spend more time

**[27:17]** You allow models to spend more time

**[27:17]** You allow models to spend more time thinking before they respond. Now, it's

**[27:19]** thinking before they respond. Now, it's

**[27:20]** thinking before they respond. Now, it's very common. we've used a lot of these

**[27:21]** very common. we've used a lot of these

**[27:21]** very common. we've used a lot of these models, right? They're starting to

**[27:22]** models, right? They're starting to

**[27:22]** models, right? They're starting to become even a little bit more agentic

**[27:24]** become even a little bit more agentic

**[27:24]** become even a little bit more agentic with RL, that's kind of what's

**[27:26]** with RL, that's kind of what's

**[27:26]** with RL, that's kind of what's progressed since we last covered this.

**[27:28]** progressed since we last covered this.

**[27:28]** progressed since we last covered this. So, um, in instead of spending hundreds

**[27:31]** So, um, in instead of spending hundreds

**[27:31]** So, um, in instead of spending hundreds of millions of dollars pre-training

**[27:33]** of millions of dollars pre-training

**[27:33]** of millions of dollars pre-training LLMs, uh, that starts to get

**[27:35]** LLMs, uh, that starts to get

**[27:35]** LLMs, uh, that starts to get exponentially more expensive, right?

**[27:37]** exponentially more expensive, right?

**[27:37]** exponentially more expensive, right? Instead of changing from hundreds of

**[27:39]** Instead of changing from hundreds of

**[27:39]** Instead of changing from hundreds of millions of dollars to pre-train a

**[27:41]** millions of dollars to pre-train a

**[27:41]** millions of dollars to pre-train a llama, we don't want to spend billions

**[27:42]** llama, we don't want to spend billions

**[27:42]** llama, we don't want to spend billions on train runs, right? So, we needed

**[27:44]** on train runs, right? So, we needed

**[27:44]** on train runs, right? So, we needed another access. So instead we shifted to

**[27:47]** another access. So instead we shifted to

**[27:47]** another access. So instead we shifted to this paradigm of inference time scaling

**[27:49]** this paradigm of inference time scaling

**[27:49]** this paradigm of inference time scaling where you take really hard questions you

**[27:52]** where you take really hard questions you

**[27:52]** where you take really hard questions you do um you know inference time chain of

**[27:54]** do um you know inference time chain of

**[27:54]** do um you know inference time chain of thought RL and we have a new dimension

**[27:57]** thought RL and we have a new dimension

**[27:57]** thought RL and we have a new dimension to on which we can scale. So previously


### [28:00 - 29:00]

**[28:00]** to on which we can scale. So previously

**[28:00]** to on which we can scale. So previously people tried to um do process b uh

**[28:04]** people tried to um do process b uh

**[28:04]** people tried to um do process b uh process based reward modeling. So we

**[28:06]** process based reward modeling. So we

**[28:06]** process based reward modeling. So we would do RL basically on um we would try

**[28:09]** would do RL basically on um we would try

**[28:09]** would do RL basically on um we would try RL, we would do beam search, we would do

**[28:12]** RL, we would do beam search, we would do

**[28:12]** RL, we would do beam search, we would do MCT uh MCTS, we would do all these

**[28:15]** MCT uh MCTS, we would do all these

**[28:15]** MCT uh MCTS, we would do all these inference time, you know, let's predict

**[28:17]** inference time, you know, let's predict

**[28:17]** inference time, you know, let's predict multiple tokens go down these processes.

**[28:19]** multiple tokens go down these processes.

**[28:19]** multiple tokens go down these processes. They were all hacks, but nothing was

**[28:21]** They were all hacks, but nothing was

**[28:21]** They were all hacks, but nothing was really close to 01, right? We would see

**[28:23]** really close to 01, right? We would see

**[28:23]** really close to 01, right? We would see Twitter demos. We would see like some

**[28:25]** Twitter demos. We would see like some

**[28:25]** Twitter demos. We would see like some fundraising that even came out of, you

**[28:26]** fundraising that even came out of, you

**[28:26]** fundraising that even came out of, you know, okay, I have much better

**[28:28]** know, okay, I have much better

**[28:28]** know, okay, I have much better performance than Llama 3 because I go

**[28:30]** performance than Llama 3 because I go

**[28:30]** performance than Llama 3 because I go down 10 trees of thought and you know,

**[28:33]** down 10 trees of thought and you know,

**[28:33]** down 10 trees of thought and you know, I'm doing all this stuff on the back end

**[28:34]** I'm doing all this stuff on the back end

**[28:34]** I'm doing all this stuff on the back end using a bunch of token and tokens and

**[28:36]** using a bunch of token and tokens and

**[28:36]** using a bunch of token and tokens and gluing it together. But this is really

**[28:39]** gluing it together. But this is really

**[28:39]** gluing it together. But this is really um not the right approach as we see.

**[28:41]** um not the right approach as we see.

**[28:41]** um not the right approach as we see. What really worked is just native pure

**[28:43]** What really worked is just native pure

**[28:43]** What really worked is just native pure beautiful scaled up RL. So um once again

**[28:48]** beautiful scaled up RL. So um once again

**[28:48]** beautiful scaled up RL. So um once again now this is what uh DeepSeek did to make

**[28:51]** now this is what uh DeepSeek did to make

**[28:51]** now this is what uh DeepSeek did to make V3. Here's the one slide if you want to

**[28:53]** V3. Here's the one slide if you want to

**[28:53]** V3. Here's the one slide if you want to know what uh DeepScync v3 is. Uh it's

**[28:56]** know what uh DeepScync v3 is. Uh it's

**[28:56]** know what uh DeepScync v3 is. Uh it's open-source GPT40

**[28:59]** open-source GPT40

**[28:59]** open-source GPT40 uh oh sorry this is Deepseek V3. This is


### [29:00 - 30:00]

**[29:01]** uh oh sorry this is Deepseek V3. This is

**[29:02]** uh oh sorry this is Deepseek V3. This is the precursor to R1. So um 40 quality 37

**[29:05]** the precursor to R1. So um 40 quality 37

**[29:05]** the precursor to R1. So um 40 quality 37 billion active parameters. This is the

**[29:07]** billion active parameters. This is the

**[29:08]** billion active parameters. This is the you know regular non-reasoning model.

**[29:10]** you know regular non-reasoning model.

**[29:10]** you know regular non-reasoning model. This is what they build R1 off of. So

**[29:12]** This is what they build R1 off of. So

**[29:12]** This is what they build R1 off of. So it's a 671 billion parameters 37 billion

**[29:17]** it's a 671 billion parameters 37 billion

**[29:17]** it's a 671 billion parameters 37 billion active. Uh they launched this it was a

**[29:19]** active. Uh they launched this it was a

**[29:19]** active. Uh they launched this it was a good model. It's just really chunky

**[29:20]** good model. It's just really chunky

**[29:20]** good model. It's just really chunky right? you can't run it on your laptop.

**[29:22]** right? you can't run it on your laptop.

**[29:22]** right? you can't run it on your laptop. No one can run a 700B model. Um they

**[29:25]** No one can run a 700B model. Um they

**[29:25]** No one can run a 700B model. Um they made this whole, you know, we're better

**[29:26]** made this whole, you know, we're better

**[29:26]** made this whole, you know, we're better than everyone else where um you know, we

**[29:28]** than everyone else where um you know, we

**[29:28]** than everyone else where um you know, we could train this model in $5 million.

**[29:31]** could train this model in $5 million.

**[29:31]** could train this model in $5 million. And I think that this is actually true,

**[29:33]** And I think that this is actually true,

**[29:33]** And I think that this is actually true, right? Looking back at it, um a lot of

**[29:35]** right? Looking back at it, um a lot of

**[29:35]** right? Looking back at it, um a lot of what we see is DeepSeek and Chinese Labs

**[29:39]** what we see is DeepSeek and Chinese Labs

**[29:39]** what we see is DeepSeek and Chinese Labs were really able to catch up to the

**[29:40]** were really able to catch up to the

**[29:40]** were really able to catch up to the United States because of the

**[29:42]** United States because of the

**[29:42]** United States because of the constraints, right? We put a lot of uh

**[29:44]** constraints, right? We put a lot of uh

**[29:44]** constraints, right? We put a lot of uh trade restrictions. We couldn't give

**[29:46]** trade restrictions. We couldn't give

**[29:46]** trade restrictions. We couldn't give them GPUs. So, they had to get clever

**[29:48]** them GPUs. So, they had to get clever

**[29:48]** them GPUs. So, they had to get clever and smart with what they got. And

**[29:50]** and smart with what they got. And

**[29:50]** and smart with what they got. And basically, you know, they they they did

**[29:52]** basically, you know, they they they did

**[29:52]** basically, you know, they they they did very very strong inference optimization.

**[29:54]** very very strong inference optimization.

**[29:54]** very very strong inference optimization. They made the most of what they had,

**[29:56]** They made the most of what they had,

**[29:56]** They made the most of what they had, right? We could have continued scaling,

**[29:58]** right? We could have continued scaling,

**[29:58]** right? We could have continued scaling, right? We could have thrown this 14.8


### [30:00 - 31:00]

**[30:01]** right? We could have thrown this 14.8

**[30:01]** right? We could have thrown this 14.8 trillion tokens into 150 trillion

**[30:03]** trillion tokens into 150 trillion

**[30:03]** trillion tokens into 150 trillion tokens, but China didn't have GPUs for

**[30:06]** tokens, but China didn't have GPUs for

**[30:06]** tokens, but China didn't have GPUs for that, right? The Deep Seek Labs were

**[30:07]** that, right? The Deep Seek Labs were

**[30:07]** that, right? The Deep Seek Labs were like, we realize that we can't scale

**[30:09]** like, we realize that we can't scale

**[30:09]** like, we realize that we can't scale this in the same dimension. So, they had

**[30:11]** this in the same dimension. So, they had

**[30:11]** this in the same dimension. So, they had to get creative and think about, okay,

**[30:13]** to get creative and think about, okay,

**[30:13]** to get creative and think about, okay, what if we do RL? And that's basically

**[30:15]** what if we do RL? And that's basically

**[30:15]** what if we do RL? And that's basically what they did. So um V3 was you know it

**[30:20]** what they did. So um V3 was you know it

**[30:20]** what they did. So um V3 was you know it was ane 37 billion active they

**[30:23]** was ane 37 billion active they

**[30:23]** was ane 37 billion active they introduced this concept of multi-headed

**[30:25]** introduced this concept of multi-headed

**[30:25]** introduced this concept of multi-headed latent attention 15 trillion tokens they

**[30:28]** latent attention 15 trillion tokens they

**[30:28]** latent attention 15 trillion tokens they did SFT and then you know traditional RL

**[30:30]** did SFT and then you know traditional RL

**[30:30]** did SFT and then you know traditional RL so RHF to make it a chat model um

**[30:33]** so RHF to make it a chat model um

**[30:33]** so RHF to make it a chat model um multi-token prediction this came out of

**[30:35]** multi-token prediction this came out of

**[30:35]** multi-token prediction this came out of meta they needed to be sample efficient

**[30:37]** meta they needed to be sample efficient

**[30:37]** meta they needed to be sample efficient with their 15 trillion tokens so

**[30:39]** with their 15 trillion tokens so

**[30:39]** with their 15 trillion tokens so multi-token prediction for a little bit

**[30:41]** multi-token prediction for a little bit

**[30:41]** multi-token prediction for a little bit more sample um efficiency trained in FBA

**[30:45]** more sample um efficiency trained in FBA

**[30:45]** more sample um efficiency trained in FBA did some long context uh extension

**[30:47]** did some long context uh extension

**[30:47]** did some long context uh extension basically first trained it at 32K then

**[30:50]** basically first trained it at 32K then

**[30:50]** basically first trained it at 32K then they extended this down to 128K came out

**[30:52]** they extended this down to 128K came out

**[30:52]** they extended this down to 128K came out a month ago from uh R1 after that R1

**[30:56]** a month ago from uh R1 after that R1

**[30:56]** a month ago from uh R1 after that R1 came out people got mad hyped now we

**[30:58]** came out people got mad hyped now we

**[30:58]** came out people got mad hyped now we have R1 v2 basically so these are kind


### [31:00 - 32:00]

**[31:02]** have R1 v2 basically so these are kind

**[31:02]** have R1 v2 basically so these are kind of you know they're fancy diagrams we

**[31:05]** of you know they're fancy diagrams we

**[31:05]** of you know they're fancy diagrams we have deepseek v3 base we do SFT and RL

**[31:09]** have deepseek v3 base we do SFT and RL

**[31:09]** have deepseek v3 base we do SFT and RL you have a SFT checkpoint RL with uh RHF

**[31:12]** you have a SFT checkpoint RL with uh RHF

**[31:12]** you have a SFT checkpoint RL with uh RHF to get uh sorry fine-tune with RL to get

**[31:15]** to get uh sorry fine-tune with RL to get

**[31:15]** to get uh sorry fine-tune with RL to get Deepseek R1. Okay. What is DeepSeek R10?

**[31:20]** Deepseek R1. Okay. What is DeepSeek R10?

**[31:20]** Deepseek R1. Okay. What is DeepSeek R10? R10 is where you don't do NF any SFT.

**[31:24]** R10 is where you don't do NF any SFT.

**[31:24]** R10 is where you don't do NF any SFT. You take a pure base model. For those

**[31:25]** You take a pure base model. For those

**[31:25]** You take a pure base model. For those that don't remember, base models are

**[31:27]** that don't remember, base models are

**[31:27]** that don't remember, base models are models that come when you do your

**[31:29]** models that come when you do your

**[31:29]** models that come when you do your pre-training. So you pertain we train

**[31:31]** pre-training. So you pertain we train

**[31:31]** pre-training. So you pertain we train models to predict the next token. Right?

**[31:33]** models to predict the next token. Right?

**[31:34]** models to predict the next token. Right? So these are not models like GPT40 or

**[31:37]** So these are not models like GPT40 or

**[31:37]** So these are not models like GPT40 or 01. These are purebased models. All they

**[31:39]** 01. These are purebased models. All they

**[31:40]** 01. These are purebased models. All they do is they predict the next token. So

**[31:42]** do is they predict the next token. So

**[31:42]** do is they predict the next token. So they're kind of completionist models,

**[31:44]** they're kind of completionist models,

**[31:44]** they're kind of completionist models, right? You can't normally chat with

**[31:46]** right? You can't normally chat with

**[31:46]** right? You can't normally chat with these. All they do is complete your

**[31:47]** these. All they do is complete your

**[31:48]** these. All they do is complete your sentence, complete your word. So they

**[31:50]** sentence, complete your word. So they

**[31:50]** sentence, complete your word. So they take the Deepseek V3 base model. They

**[31:52]** take the Deepseek V3 base model. They

**[31:52]** take the Deepseek V3 base model. They apply pure RL. They don't do any SFT.

**[31:55]** apply pure RL. They don't do any SFT.

**[31:55]** apply pure RL. They don't do any SFT. They don't train it as a user assistant

**[31:57]** They don't train it as a user assistant

**[31:57]** They don't train it as a user assistant chat model. They don't do any of that.

**[31:59]** chat model. They don't do any of that.

**[31:59]** chat model. They don't do any of that. It uses gRPO for RL which they, you


### [32:00 - 33:00]

**[32:01]** It uses gRPO for RL which they, you

**[32:01]** It uses gRPO for RL which they, you know, they actually introduce quite a

**[32:03]** know, they actually introduce quite a

**[32:03]** know, they actually introduce quite a while in Deepseek math. The reward is

**[32:05]** while in Deepseek math. The reward is

**[32:05]** while in Deepseek math. The reward is based on both accuracy and format.

**[32:07]** based on both accuracy and format.

**[32:07]** based on both accuracy and format. Responses must be verifiably correct.

**[32:10]** Responses must be verifiably correct.

**[32:10]** Responses must be verifiably correct. Right? So what are they doing here? They

**[32:11]** Right? So what are they doing here? They

**[32:12]** Right? So what are they doing here? They take a base non-hat model. They use gpo

**[32:15]** take a base non-hat model. They use gpo

**[32:15]** take a base non-hat model. They use gpo style RL on math and code and they have

**[32:17]** style RL on math and code and they have

**[32:17]** style RL on math and code and they have a verifiable output right so the models

**[32:20]** a verifiable output right so the models

**[32:20]** a verifiable output right so the models need to be verifiably output they need

**[32:23]** need to be verifiably output they need

**[32:23]** need to be verifiably output they need their output to be verifiably correct

**[32:24]** their output to be verifiably correct

**[32:24]** their output to be verifiably correct right so for math that's you have a

**[32:27]** right so for math that's you have a

**[32:27]** right so for math that's you have a correct output to your math question

**[32:28]** correct output to your math question

**[32:28]** correct output to your math question right is the answer correct or not

**[32:30]** right is the answer correct or not

**[32:30]** right is the answer correct or not correct if it's correct that's good we

**[32:31]** correct if it's correct that's good we

**[32:32]** correct if it's correct that's good we can RL on that for code does your code

**[32:34]** can RL on that for code does your code

**[32:34]** can RL on that for code does your code compile if it compiles that's good we

**[32:36]** compile if it compiles that's good we

**[32:36]** compile if it compiles that's good we can do RL on that and you know this is

**[32:38]** can do RL on that and you know this is

**[32:38]** can do RL on that and you know this is leak code style questions so le code

**[32:41]** leak code style questions so le code

**[32:41]** leak code style questions so le code style questions we know the answer we

**[32:43]** style questions we know the answer we

**[32:43]** style questions we know the answer we know if the answer is correct. Uh then

**[32:45]** know if the answer is correct. Uh then

**[32:45]** know if the answer is correct. Uh then we format the uh basically you know in

**[32:47]** we format the uh basically you know in

**[32:47]** we format the uh basically you know in the little minute details they format

**[32:49]** the little minute details they format

**[32:49]** the little minute details they format the rewards they kind of output the

**[32:51]** the rewards they kind of output the

**[32:51]** the rewards they kind of output the thinking between think tags. So we take

**[32:53]** thinking between think tags. So we take

**[32:53]** thinking between think tags. So we take our base model we do RL to do a bunch of

**[32:55]** our base model we do RL to do a bunch of

**[32:56]** our base model we do RL to do a bunch of thinking to get its chain of thought

**[32:57]** thinking to get its chain of thought

**[32:57]** thinking to get its chain of thought thinking process and from there we kind

**[32:59]** thinking process and from there we kind

**[32:59]** thinking process and from there we kind of have deepse R10. It's a it's a


### [33:00 - 34:00]

**[33:03]** of have deepse R10. It's a it's a

**[33:03]** of have deepse R10. It's a it's a reasoning thinking model that's good at

**[33:05]** reasoning thinking model that's good at

**[33:05]** reasoning thinking model that's good at outputting thinking and answers but it

**[33:08]** outputting thinking and answers but it

**[33:08]** outputting thinking and answers but it hasn't really been trained to be a

**[33:10]** hasn't really been trained to be a

**[33:10]** hasn't really been trained to be a useful assistant. It's not 01 yet. This

**[33:13]** useful assistant. It's not 01 yet. This

**[33:13]** useful assistant. It's not 01 yet. This is just a good thinking model that can

**[33:14]** is just a good thinking model that can

**[33:14]** is just a good thinking model that can unravel its thought process to generate

**[33:17]** unravel its thought process to generate

**[33:17]** unravel its thought process to generate out answers. We'll probably skip this

**[33:19]** out answers. We'll probably skip this

**[33:19]** out answers. We'll probably skip this guide. This is a gpo. This is kind of

**[33:22]** guide. This is a gpo. This is kind of

**[33:22]** guide. This is a gpo. This is kind of the RL based algorithm that they use.

**[33:25]** the RL based algorithm that they use.

**[33:25]** the RL based algorithm that they use. It's you know comparing PO GRPO. This is

**[33:28]** It's you know comparing PO GRPO. This is

**[33:28]** It's you know comparing PO GRPO. This is how they do the RL. We'll kind of skip

**[33:30]** how they do the RL. We'll kind of skip

**[33:30]** how they do the RL. We'll kind of skip it. The key the key things to note

**[33:32]** it. The key the key things to note

**[33:32]** it. The key the key things to note there's no critique model. Uh you have

**[33:34]** there's no critique model. Uh you have

**[33:34]** there's no critique model. Uh you have group based rewards that are scored. Uh

**[33:37]** group based rewards that are scored. Uh

**[33:37]** group based rewards that are scored. Uh it has stability updates. There's a, you

**[33:39]** it has stability updates. There's a, you

**[33:39]** it has stability updates. There's a, you know, KL divergence, but yeah, they did

**[33:42]** know, KL divergence, but yeah, they did

**[33:42]** know, KL divergence, but yeah, they did they do GRPO. We're going to skip it in

**[33:43]** they do GRPO. We're going to skip it in

**[33:43]** they do GRPO. We're going to skip it in this talk for now. So, okay, Deepseek

**[33:47]** this talk for now. So, okay, Deepseek

**[33:47]** this talk for now. So, okay, Deepseek R10, we took a base model, we did RL, we

**[33:51]** R10, we took a base model, we did RL, we

**[33:51]** R10, we took a base model, we did RL, we now have a thinking model. How does it

**[33:53]** now have a thinking model. How does it

**[33:53]** now have a thinking model. How does it perform? Pretty good. Uh, aime, you

**[33:55]** perform? Pretty good. Uh, aime, you

**[33:55]** perform? Pretty good. Uh, aime, you know, it passes 01 mini for the time.

**[33:58]** know, it passes 01 mini for the time.

**[33:58]** know, it passes 01 mini for the time. Uh, math, it passes math 500, it passes


### [34:00 - 35:00]

**[34:00]** Uh, math, it passes math 500, it passes

**[34:00]** Uh, math, it passes math 500, it passes 01 mini. It's not, it's like on par with

**[34:03]** 01 mini. It's not, it's like on par with

**[34:03]** 01 mini. It's not, it's like on par with but slightly worse than 01. And then you

**[34:05]** but slightly worse than 01. And then you

**[34:05]** but slightly worse than 01. And then you know the charts show that um we're able

**[34:09]** know the charts show that um we're able

**[34:09]** know the charts show that um we're able to do this inference time scaling by

**[34:11]** to do this inference time scaling by

**[34:11]** to do this inference time scaling by doing RL on really hard questions and it

**[34:14]** doing RL on really hard questions and it

**[34:14]** doing RL on really hard questions and it kind of works. It works pretty well. So

**[34:17]** kind of works. It works pretty well. So

**[34:17]** kind of works. It works pretty well. So um yes chart number goes up number goes

**[34:19]** um yes chart number goes up number goes

**[34:20]** um yes chart number goes up number goes up even more the more you train this

**[34:22]** up even more the more you train this

**[34:22]** up even more the more you train this thing is starting to stably learn. Um

**[34:24]** thing is starting to stably learn. Um

**[34:24]** thing is starting to stably learn. Um what else do we learn? So it naturally

**[34:27]** what else do we learn? So it naturally

**[34:27]** what else do we learn? So it naturally has the ability to solve these complex

**[34:29]** has the ability to solve these complex

**[34:29]** has the ability to solve these complex tasks by extending test time compute.

**[34:31]** tasks by extending test time compute.

**[34:31]** tasks by extending test time compute. Right here we know that the original

**[34:33]** Right here we know that the original

**[34:33]** Right here we know that the original R10, it ranged from hundreds to

**[34:35]** R10, it ranged from hundreds to

**[34:35]** R10, it ranged from hundreds to thousands of reasoning tokens. Turns out

**[34:37]** thousands of reasoning tokens. Turns out

**[34:37]** thousands of reasoning tokens. Turns out that this was a pretty key uh factor. In

**[34:40]** that this was a pretty key uh factor. In

**[34:40]** that this was a pretty key uh factor. In the update that they released that DeepS

**[34:42]** the update that they released that DeepS

**[34:42]** the update that they released that DeepS released last week, we scaled from

**[34:44]** released last week, we scaled from

**[34:44]** released last week, we scaled from training on uh from taking thousands of

**[34:47]** training on uh from taking thousands of

**[34:47]** training on uh from taking thousands of tokens to now taking to doubling it. So

**[34:50]** tokens to now taking to doubling it. So

**[34:50]** tokens to now taking to doubling it. So now we take from 12,000 tokens on

**[34:52]** now we take from 12,000 tokens on

**[34:52]** now we take from 12,000 tokens on average to 24,000 tokens. And once again

**[34:55]** average to 24,000 tokens. And once again

**[34:55]** average to 24,000 tokens. And once again we shift from getting 01 level to 03

**[34:57]** we shift from getting 01 level to 03

**[34:57]** we shift from getting 01 level to 03 level performance in the new DeepC um

**[34:59]** level performance in the new DeepC um

**[34:59]** level performance in the new DeepC um model. So other stuff um you know


### [35:00 - 36:00]

**[35:03]** model. So other stuff um you know

**[35:03]** model. So other stuff um you know there's this emergence of interesting

**[35:05]** there's this emergence of interesting

**[35:05]** there's this emergence of interesting behaviors as test time compute

**[35:07]** behaviors as test time compute

**[35:07]** behaviors as test time compute increases. So as we're able to increase

**[35:10]** increases. So as we're able to increase

**[35:10]** increases. So as we're able to increase our test time training as we can reason

**[35:12]** our test time training as we can reason

**[35:12]** our test time training as we can reason for more and more time we start to see

**[35:15]** for more and more time we start to see

**[35:15]** for more and more time we start to see this emergence of interesting behaviors.

**[35:18]** this emergence of interesting behaviors.

**[35:18]** this emergence of interesting behaviors. Basically as models learn to reason for

**[35:21]** Basically as models learn to reason for

**[35:21]** Basically as models learn to reason for longer and longer as you get to

**[35:23]** longer and longer as you get to

**[35:23]** longer and longer as you get to thousands of steps of reasoning models

**[35:25]** thousands of steps of reasoning models

**[35:25]** thousands of steps of reasoning models are able to start to have these

**[35:26]** are able to start to have these

**[35:26]** are able to start to have these reflection moments. So you know the more

**[35:29]** reflection moments. So you know the more

**[35:29]** reflection moments. So you know the more reasoning you do models start to learn

**[35:31]** reasoning you do models start to learn

**[35:31]** reasoning you do models start to learn okay I'm actually not forced to just

**[35:33]** okay I'm actually not forced to just

**[35:33]** okay I'm actually not forced to just output my next token I'm not forced to

**[35:35]** output my next token I'm not forced to

**[35:35]** output my next token I'm not forced to do a 100 tokens of thinking I'm not

**[35:37]** do a 100 tokens of thinking I'm not

**[35:37]** do a 100 tokens of thinking I'm not forced to do a thousand I can continue

**[35:39]** forced to do a thousand I can continue

**[35:39]** forced to do a thousand I can continue down this thinking path and we notice

**[35:41]** down this thinking path and we notice

**[35:41]** down this thinking path and we notice that you know as they start to reason

**[35:43]** that you know as they start to reason

**[35:43]** that you know as they start to reason for longer models start to do this sort

**[35:46]** for longer models start to do this sort

**[35:46]** for longer models start to do this sort of reflection phase models start to re

**[35:48]** of reflection phase models start to re

**[35:48]** of reflection phase models start to re revisit re-evaluate their previous steps

**[35:51]** revisit re-evaluate their previous steps

**[35:51]** revisit re-evaluate their previous steps they start to go down alternative paths

**[35:53]** they start to go down alternative paths

**[35:53]** they start to go down alternative paths and you know this kind of arises this

**[35:55]** and you know this kind of arises this

**[35:55]** and you know this kind of arises this spontaneity um of this spontaneuity

**[35:59]** spontaneity um of this spontaneuity

**[35:59]** spontaneity um of this spontaneuity emergence right so spontaneously you


### [36:00 - 37:00]

**[36:01]** emergence right so spontaneously you

**[36:01]** emergence right so spontaneously you know models will be like okay I tried

**[36:03]** know models will be like okay I tried

**[36:03]** know models will be like okay I tried this this this it's not working I don't

**[36:05]** this this this it's not working I don't

**[36:05]** this this this it's not working I don't have to answer right now let me try this

**[36:07]** have to answer right now let me try this

**[36:07]** have to answer right now let me try this new thing and guess what it works and

**[36:09]** new thing and guess what it works and

**[36:09]** new thing and guess what it works and then we also had these aha moments so

**[36:12]** then we also had these aha moments so

**[36:12]** then we also had these aha moments so very core takeaway of the um paper you

**[36:15]** very core takeaway of the um paper you

**[36:15]** very core takeaway of the um paper you know this is kind of what got the

**[36:17]** know this is kind of what got the

**[36:17]** know this is kind of what got the deepseeek authors to realize okay RL

**[36:19]** deepseeek authors to realize okay RL

**[36:19]** deepseeek authors to realize okay RL actually works um the more time that we

**[36:22]** actually works um the more time that we

**[36:22]** actually works um the more time that we think for the the further along the

**[36:25]** think for the the further along the

**[36:25]** think for the the further along the thinking traces we start to get these

**[36:27]** thinking traces we start to get these

**[36:27]** thinking traces we start to get these models to do these aha moments. So

**[36:29]** models to do these aha moments. So

**[36:29]** models to do these aha moments. So here's a quote again from the paper.

**[36:31]** here's a quote again from the paper.

**[36:31]** here's a quote again from the paper. This moment is not only an aha moment

**[36:33]** This moment is not only an aha moment

**[36:33]** This moment is not only an aha moment for the model but also for the

**[36:34]** for the model but also for the

**[36:34]** for the model but also for the researchers observing its behavior. It

**[36:37]** researchers observing its behavior. It

**[36:37]** researchers observing its behavior. It underscores the power and beauty of

**[36:39]** underscores the power and beauty of

**[36:39]** underscores the power and beauty of reinforcement learning. Rather than

**[36:41]** reinforcement learning. Rather than

**[36:41]** reinforcement learning. Rather than explicitly teaching the model on how to

**[36:43]** explicitly teaching the model on how to

**[36:43]** explicitly teaching the model on how to solve a problem, we simply provide it

**[36:46]** solve a problem, we simply provide it

**[36:46]** solve a problem, we simply provide it with the right incentives and it

**[36:47]** with the right incentives and it

**[36:47]** with the right incentives and it autonomously develops advanced problem

**[36:49]** autonomously develops advanced problem

**[36:49]** autonomously develops advanced problem solving strategies. The aha moment

**[36:52]** solving strategies. The aha moment

**[36:52]** solving strategies. The aha moment serves as a powerful reminder of the

**[36:54]** serves as a powerful reminder of the

**[36:54]** serves as a powerful reminder of the potential of reinforcement that

**[36:56]** potential of reinforcement that

**[36:56]** potential of reinforcement that reinforcement learning un unlocks new

**[36:58]** reinforcement learning un unlocks new

**[36:58]** reinforcement learning un unlocks new levels of intelligence and AI systems.


### [37:00 - 38:00]

**[37:00]** levels of intelligence and AI systems.

**[37:00]** levels of intelligence and AI systems. So basically instead of telling models

**[37:04]** So basically instead of telling models

**[37:04]** So basically instead of telling models how to reason, what traces, instead of

**[37:06]** how to reason, what traces, instead of

**[37:06]** how to reason, what traces, instead of doing SFT on chain of thought traces, if

**[37:09]** doing SFT on chain of thought traces, if

**[37:09]** doing SFT on chain of thought traces, if we just give it this sparse idea of, you

**[37:11]** we just give it this sparse idea of, you

**[37:11]** we just give it this sparse idea of, you know, reason as much as you want, um the

**[37:14]** know, reason as much as you want, um the

**[37:14]** know, reason as much as you want, um the more time it takes, we start to notice

**[37:15]** more time it takes, we start to notice

**[37:15]** more time it takes, we start to notice emergence of aha, I see what it should

**[37:18]** emergence of aha, I see what it should

**[37:18]** emergence of aha, I see what it should finally be. I tried these six steps and

**[37:20]** finally be. I tried these six steps and

**[37:20]** finally be. I tried these six steps and this seventh step is working. And you

**[37:22]** this seventh step is working. And you

**[37:22]** this seventh step is working. And you know, this shows the power of RL. And

**[37:24]** know, this shows the power of RL. And

**[37:24]** know, this shows the power of RL. And this has been kind of passed down into

**[37:26]** this has been kind of passed down into

**[37:26]** this has been kind of passed down into other papers. Microsoft has put out

**[37:28]** other papers. Microsoft has put out

**[37:28]** other papers. Microsoft has put out really good scaling laws on doing RL on

**[37:30]** really good scaling laws on doing RL on

**[37:30]** really good scaling laws on doing RL on small models. Uh Quen team has done

**[37:32]** small models. Uh Quen team has done

**[37:32]** small models. Uh Quen team has done really good thinking models. They have

**[37:34]** really good thinking models. They have

**[37:34]** really good thinking models. They have an online talk here. So please please

**[37:36]** an online talk here. So please please

**[37:36]** an online talk here. So please please watch out the Quen reasoning talk. Uh a

**[37:39]** watch out the Quen reasoning talk. Uh a

**[37:39]** watch out the Quen reasoning talk. Uh a speaker speaks about how they do that.

**[37:40]** speaker speaks about how they do that.

**[37:40]** speaker speaks about how they do that. But okay back to Deep Seek. Back to Deep

**[37:43]** But okay back to Deep Seek. Back to Deep

**[37:43]** But okay back to Deep Seek. Back to Deep Seek. Um this is an example of an aha

**[37:46]** Seek. Um this is an example of an aha

**[37:46]** Seek. Um this is an example of an aha moment. So question uh basic math

**[37:49]** moment. So question uh basic math

**[37:49]** moment. So question uh basic math question you know not not that basic

**[37:51]** question you know not not that basic

**[37:51]** question you know not not that basic actually. I can't answer this. If a is

**[37:53]** actually. I can't answer this. If a is

**[37:53]** actually. I can't answer this. If a is greater than one, then the sum of this

**[37:55]** greater than one, then the sum of this

**[37:55]** greater than one, then the sum of this square root of a square root is equal to

**[37:56]** square root of a square root is equal to

**[37:56]** square root of a square root is equal to something. Well, as the model starts to

**[37:59]** something. Well, as the model starts to

**[37:59]** something. Well, as the model starts to think, you know, it realizes, oh,


### [38:00 - 39:00]

**[38:01]** think, you know, it realizes, oh,

**[38:01]** think, you know, it realizes, oh, there's a x squar, right? If I square

**[38:03]** there's a x squar, right? If I square

**[38:03]** there's a x squar, right? If I square this, I can get x squared. Then I can

**[38:05]** this, I can get x squared. Then I can

**[38:05]** this, I can get x squared. Then I can isolate this out, right? It's doing

**[38:07]** isolate this out, right? It's doing

**[38:07]** isolate this out, right? It's doing reasoning. It's thinking through this

**[38:08]** reasoning. It's thinking through this

**[38:08]** reasoning. It's thinking through this math problem step by step. Then it says

**[38:10]** math problem step by step. Then it says

**[38:10]** math problem step by step. Then it says this in its own generation. Wait, wait,

**[38:13]** this in its own generation. Wait, wait,

**[38:13]** this in its own generation. Wait, wait, wait. That's an aha moment. I can flag

**[38:15]** wait. That's an aha moment. I can flag

**[38:15]** wait. That's an aha moment. I can flag here. Let's re-evaluate this. Oh, okay.

**[38:17]** here. Let's re-evaluate this. Oh, okay.

**[38:18]** here. Let's re-evaluate this. Oh, okay. instead of you know actually solving the

**[38:19]** instead of you know actually solving the

**[38:19]** instead of you know actually solving the square root. I can do this square both

**[38:22]** square root. I can do this square both

**[38:22]** square root. I can do this square both side and like you know very interesting

**[38:24]** side and like you know very interesting

**[38:24]** side and like you know very interesting aha moments start to pop up. So yeah um

**[38:28]** aha moments start to pop up. So yeah um

**[38:28]** aha moments start to pop up. So yeah um that's kind of overview of what R1 was.

**[38:31]** that's kind of overview of what R1 was.

**[38:31]** that's kind of overview of what R1 was. Uh that's kind of an overview of R what

**[38:33]** Uh that's kind of an overview of R what

**[38:33]** Uh that's kind of an overview of R what R10 was. R10 is basically where we take

**[38:37]** R10 was. R10 is basically where we take

**[38:37]** R10 was. R10 is basically where we take a base model we train it on pure RL for

**[38:40]** a base model we train it on pure RL for

**[38:40]** a base model we train it on pure RL for math and code. We do RL on thinking

**[38:43]** math and code. We do RL on thinking

**[38:43]** math and code. We do RL on thinking steps and now we have a reasoning model.

**[38:45]** steps and now we have a reasoning model.

**[38:45]** steps and now we have a reasoning model. It works, but um it's it's not great. Uh

**[38:48]** It works, but um it's it's not great. Uh

**[38:48]** It works, but um it's it's not great. Uh it doesn't actually have great re

**[38:50]** it doesn't actually have great re

**[38:50]** it doesn't actually have great re readability. It starts to reason in

**[38:53]** readability. It starts to reason in

**[38:53]** readability. It starts to reason in multiple languages. Whoa, it reasoned in

**[38:55]** multiple languages. Whoa, it reasoned in

**[38:55]** multiple languages. Whoa, it reasoned in Chinese. Who would have guessed? So um

**[38:57]** Chinese. Who would have guessed? So um

**[38:57]** Chinese. Who would have guessed? So um we want to make it, you know, we can't

**[38:59]** we want to make it, you know, we can't

**[38:59]** we want to make it, you know, we can't just skip RHF, right? We want to make


### [39:00 - 40:00]

**[39:01]** just skip RHF, right? We want to make

**[39:01]** just skip RHF, right? We want to make this thing a good chat model. So next

**[39:04]** this thing a good chat model. So next

**[39:04]** this thing a good chat model. So next section, instead of R10, how do we make

**[39:07]** section, instead of R10, how do we make

**[39:07]** section, instead of R10, how do we make Deepseek R1? How do we take Deepseek R1

**[39:10]** Deepseek R1? How do we take Deepseek R1

**[39:10]** Deepseek R1? How do we take Deepseek R1 from being just a reasoning model to a

**[39:13]** from being just a reasoning model to a

**[39:13]** from being just a reasoning model to a reasoning useful assistant? that's good

**[39:15]** reasoning useful assistant? that's good

**[39:15]** reasoning useful assistant? that's good at you know actually being a chat full

**[39:17]** at you know actually being a chat full

**[39:17]** at you know actually being a chat full assistant. So key solution giving it to

**[39:20]** assistant. So key solution giving it to

**[39:20]** assistant. So key solution giving it to you straight cold start instead of

**[39:23]** you straight cold start instead of

**[39:23]** you straight cold start instead of taking base model to just RL take a base

**[39:26]** taking base model to just RL take a base

**[39:26]** taking base model to just RL take a base model do some regular SFT SFT is kind of

**[39:30]** model do some regular SFT SFT is kind of

**[39:30]** model do some regular SFT SFT is kind of you know here's some prompt answer

**[39:31]** you know here's some prompt answer

**[39:31]** you know here's some prompt answer here's chat assistant get it to you know

**[39:34]** here's chat assistant get it to you know

**[39:34]** here's chat assistant get it to you know do a cold start get it to understand

**[39:35]** do a cold start get it to understand

**[39:35]** do a cold start get it to understand you're still a useful assistant after

**[39:37]** you're still a useful assistant after

**[39:37]** you're still a useful assistant after that do some RL do this core RL on very

**[39:41]** that do some RL do this core RL on very

**[39:41]** that do some RL do this core RL on very hard code math get it to start to

**[39:43]** hard code math get it to start to

**[39:43]** hard code math get it to start to understand how to think how to reason

**[39:45]** understand how to think how to reason

**[39:45]** understand how to think how to reason scale it out until it starts to see

**[39:47]** scale it out until it starts to see

**[39:47]** scale it out until it starts to see these aha moment ments until they can do

**[39:49]** these aha moment ments until they can do

**[39:49]** these aha moment ments until they can do proper reasoning. From there, let's do

**[39:51]** proper reasoning. From there, let's do

**[39:51]** proper reasoning. From there, let's do some rejection sampling. We want to take

**[39:53]** some rejection sampling. We want to take

**[39:53]** some rejection sampling. We want to take examples that don't work, right? Stuff

**[39:55]** examples that don't work, right? Stuff

**[39:55]** examples that don't work, right? Stuff where it is going wrong, where it has

**[39:57]** where it is going wrong, where it has

**[39:57]** where it is going wrong, where it has negative behavior. We do rejection

**[39:59]** negative behavior. We do rejection

**[39:59]** negative behavior. We do rejection sampling on it. And then once again, we


### [40:00 - 41:00]

**[40:01]** sampling on it. And then once again, we

**[40:01]** sampling on it. And then once again, we do it once again our last session of

**[40:03]** do it once again our last session of

**[40:03]** do it once again our last session of stage four training, which is another

**[40:04]** stage four training, which is another

**[40:04]** stage four training, which is another round of RL. So going through these,

**[40:08]** round of RL. So going through these,

**[40:08]** round of RL. So going through these, stage one, cold start with strong SFT

**[40:10]** stage one, cold start with strong SFT

**[40:10]** stage one, cold start with strong SFT prevents the model from getting

**[40:11]** prevents the model from getting

**[40:11]** prevents the model from getting unstable. So basically you take a base

**[40:14]** unstable. So basically you take a base

**[40:14]** unstable. So basically you take a base model you have a long chain of thought

**[40:16]** model you have a long chain of thought

**[40:16]** model you have a long chain of thought style um data set. So you know prompt

**[40:19]** style um data set. So you know prompt

**[40:19]** style um data set. So you know prompt answer pair so user assistant think

**[40:22]** answer pair so user assistant think

**[40:22]** answer pair so user assistant think through your process of how to solve

**[40:24]** through your process of how to solve

**[40:24]** through your process of how to solve this give your chain of thought we take

**[40:26]** this give your chain of thought we take

**[40:26]** this give your chain of thought we take a base model train it on this chain of

**[40:28]** a base model train it on this chain of

**[40:28]** a base model train it on this chain of thought then from there um you know we

**[40:31]** thought then from there um you know we

**[40:31]** thought then from there um you know we do our cold start with strong strong sft

**[40:33]** do our cold start with strong strong sft

**[40:34]** do our cold start with strong strong sft they don't just do uh synthetic data

**[40:36]** they don't just do uh synthetic data

**[40:36]** they don't just do uh synthetic data they have human annotators we want

**[40:38]** they have human annotators we want

**[40:38]** they have human annotators we want better reason we uh we want better

**[40:40]** better reason we uh we want better

**[40:40]** better reason we uh we want better readability right so do your thinking in

**[40:43]** readability right so do your thinking in

**[40:43]** readability right so do your thinking in think tags um and then this is on the

**[40:45]** think tags um and then this is on the

**[40:45]** think tags um and then this is on the scale of a couple thousand examples so

**[40:47]** scale of a couple thousand examples so

**[40:47]** scale of a couple thousand examples so base model thousand examples couple

**[40:50]** base model thousand examples couple

**[40:50]** base model thousand examples couple thousand examples of SFT then our main

**[40:52]** thousand examples of SFT then our main

**[40:52]** thousand examples of SFT then our main RL stage you know so same RL process as

**[40:56]** RL stage you know so same RL process as

**[40:56]** RL stage you know so same RL process as R10 do a lot of RL on verifiable hard


### [41:00 - 42:00]

**[41:00]** R10 do a lot of RL on verifiable hard

**[41:00]** R10 do a lot of RL on verifiable hard questions so um math leak code style

**[41:03]** questions so um math leak code style

**[41:03]** questions so um math leak code style coding stuff that we can verify has the

**[41:05]** coding stuff that we can verify has the

**[41:05]** coding stuff that we can verify has the right answer do a lot of RL and then you

**[41:07]** right answer do a lot of RL and then you

**[41:07]** right answer do a lot of RL and then you know stage three rejection sampling so

**[41:10]** know stage three rejection sampling so

**[41:10]** know stage three rejection sampling so generate completions rank them with a

**[41:13]** generate completions rank them with a

**[41:13]** generate completions rank them with a reward model fine-tune the original

**[41:15]** reward model fine-tune the original

**[41:15]** reward model fine-tune the original model so this was uh standard So Llama 3

**[41:18]** model so this was uh standard So Llama 3

**[41:18]** model so this was uh standard So Llama 3 showed us this concept of rejection

**[41:19]** showed us this concept of rejection

**[41:19]** showed us this concept of rejection sampling and you know we take it to

**[41:21]** sampling and you know we take it to

**[41:21]** sampling and you know we take it to deepsek. We do rejection sampling on

**[41:24]** deepsek. We do rejection sampling on

**[41:24]** deepsek. We do rejection sampling on samples that we don't like. We had

**[41:25]** samples that we don't like. We had

**[41:25]** samples that we don't like. We had 800,000 samples that were generated,

**[41:27]** 800,000 samples that were generated,

**[41:28]** 800,000 samples that were generated, 600,000 reasoning, 200,000 general chat.

**[41:31]** 600,000 reasoning, 200,000 general chat.

**[41:31]** 600,000 reasoning, 200,000 general chat. We rank them and then we do our

**[41:32]** We rank them and then we do our

**[41:32]** We rank them and then we do our rejection sampling training. Then final

**[41:35]** rejection sampling training. Then final

**[41:35]** rejection sampling training. Then final RL stage for general use. This is very

**[41:37]** RL stage for general use. This is very

**[41:37]** RL stage for general use. This is very similar to um you know similar to RLHF.

**[41:41]** similar to um you know similar to RLHF.

**[41:41]** similar to um you know similar to RLHF. You want to make the model helpful,

**[41:43]** You want to make the model helpful,

**[41:44]** You want to make the model helpful, harmless and reason good. So for

**[41:46]** harmless and reason good. So for

**[41:46]** harmless and reason good. So for reasoning we use you know we want to

**[41:48]** reasoning we use you know we want to

**[41:48]** reasoning we use you know we want to keep it a good reasoner add in that

**[41:50]** keep it a good reasoner add in that

**[41:50]** keep it a good reasoner add in that reasoning for hard math questions code

**[41:52]** reasoning for hard math questions code

**[41:52]** reasoning for hard math questions code questions for general chat capture human

**[41:55]** questions for general chat capture human

**[41:55]** questions for general chat capture human preference and nuance situation. So you

**[41:57]** preference and nuance situation. So you

**[41:57]** preference and nuance situation. So you know this question should have a very

**[41:59]** know this question should have a very


### [42:00 - 43:00]

**[42:00]** know this question should have a very verbose detailed answer this is a basic

**[42:02]** verbose detailed answer this is a basic

**[42:02]** verbose detailed answer this is a basic summary keep it short but keep your

**[42:04]** summary keep it short but keep your

**[42:04]** summary keep it short but keep your reasoning there. So final stage is um

**[42:07]** reasoning there. So final stage is um

**[42:07]** reasoning there. So final stage is um you know this final stage of RL and now

**[42:09]** you know this final stage of RL and now

**[42:09]** you know this final stage of RL and now guess what model is good at being a chat

**[42:12]** guess what model is good at being a chat

**[42:12]** guess what model is good at being a chat model is good at thinking model has

**[42:14]** model is good at thinking model has

**[42:14]** model is good at thinking model has emergence of aha behaviors and um the

**[42:17]** emergence of aha behaviors and um the

**[42:17]** emergence of aha behaviors and um the model is just a good chat model

**[42:19]** model is just a good chat model

**[42:19]** model is just a good chat model performance and evals we're going to

**[42:21]** performance and evals we're going to

**[42:21]** performance and evals we're going to skip this basically um when we launched

**[42:23]** skip this basically um when we launched

**[42:23]** skip this basically um when we launched deepseek it was 01 level but forget that

**[42:26]** deepseek it was 01 level but forget that

**[42:26]** deepseek it was 01 level but forget that we launched an update um two weeks ago

**[42:29]** we launched an update um two weeks ago

**[42:29]** we launched an update um two weeks ago so new model DeepS R1 launched May 28th

**[42:34]** so new model DeepS R1 launched May 28th

**[42:34]** so new model DeepS R1 launched May 28th And instead of being 01 level, the new

**[42:36]** And instead of being 01 level, the new

**[42:36]** And instead of being 01 level, the new DeepSeek R1 model is now reasoning for

**[42:39]** DeepSeek R1 model is now reasoning for

**[42:39]** DeepSeek R1 model is now reasoning for twice as long. It's as good as 03 and

**[42:42]** twice as long. It's as good as 03 and

**[42:42]** twice as long. It's as good as 03 and Gemini 2.5 Pro. Much better at math,

**[42:45]** Gemini 2.5 Pro. Much better at math,

**[42:45]** Gemini 2.5 Pro. Much better at math, much better at coding, much better at

**[42:47]** much better at coding, much better at

**[42:47]** much better at coding, much better at reasoning. It now has support for native

**[42:49]** reasoning. It now has support for native

**[42:49]** reasoning. It now has support for native function calling, JSON outputs, no

**[42:51]** function calling, JSON outputs, no

**[42:52]** function calling, JSON outputs, no longer hallucinates as much. Second

**[42:54]** longer hallucinates as much. Second

**[42:54]** longer hallucinates as much. Second model drop, and of course, you know,

**[42:55]** model drop, and of course, you know,

**[42:56]** model drop, and of course, you know, performance charts, all the regular

**[42:57]** performance charts, all the regular

**[42:57]** performance charts, all the regular benchmarks you would expect. Model is

**[42:59]** benchmarks you would expect. Model is

**[42:59]** benchmarks you would expect. Model is performing as good as 03 and Gemini 2.5.


### [43:00 - 44:00]

**[43:02]** performing as good as 03 and Gemini 2.5.

**[43:02]** performing as good as 03 and Gemini 2.5. Now all that was done do more RL uh

**[43:06]** Now all that was done do more RL uh

**[43:06]** Now all that was done do more RL uh Aimeme jumped you know 17.5%

**[43:10]** Aimeme jumped you know 17.5%

**[43:10]** Aimeme jumped you know 17.5% doubled the reasoning tokens basically

**[43:12]** doubled the reasoning tokens basically

**[43:12]** doubled the reasoning tokens basically we we doubled our reasoning access we

**[43:15]** we we doubled our reasoning access we

**[43:15]** we we doubled our reasoning access we now reason for twice as long so double

**[43:17]** now reason for twice as long so double

**[43:17]** now reason for twice as long so double the reasoning effort and um yeah now

**[43:20]** the reasoning effort and um yeah now

**[43:20]** the reasoning effort and um yeah now we're 03 and Gemini 2.5 level the other

**[43:23]** we're 03 and Gemini 2.5 level the other

**[43:23]** we're 03 and Gemini 2.5 level the other drop we have uh a new distillation we

**[43:25]** drop we have uh a new distillation we

**[43:25]** drop we have uh a new distillation we take our new model that reasons for

**[43:27]** take our new model that reasons for

**[43:27]** take our new model that reasons for twice as long we distill this down to

**[43:30]** twice as long we distill this down to

**[43:30]** twice as long we distill this down to quen 38B and We do a distillation loss

**[43:33]** quen 38B and We do a distillation loss

**[43:33]** quen 38B and We do a distillation loss on this reasoning and we get performance

**[43:36]** on this reasoning and we get performance

**[43:36]** on this reasoning and we get performance matching the Quinn 235 billion reasoning

**[43:39]** matching the Quinn 235 billion reasoning

**[43:39]** matching the Quinn 235 billion reasoning model. So our dense 8B non-reasoning

**[43:43]** model. So our dense 8B non-reasoning

**[43:43]** model. So our dense 8B non-reasoning model that was distilled from our new

**[43:46]** model that was distilled from our new

**[43:46]** model that was distilled from our new deepseek model is as good as um Quen 3

**[43:50]** deepseek model is as good as um Quen 3

**[43:50]** deepseek model is as good as um Quen 3 235B which is ane reasoning model which

**[43:53]** 235B which is ane reasoning model which

**[43:53]** 235B which is ane reasoning model which is pretty crazy. You know the the

**[43:56]** is pretty crazy. You know the the

**[43:56]** is pretty crazy. You know the the implications of this show that you know

**[43:59]** implications of this show that you know

**[43:59]** implications of this show that you know long detailed good reasoning really has


### [44:00 - 45:00]

**[44:01]** long detailed good reasoning really has

**[44:01]** long detailed good reasoning really has a deep impact. Once again check out the

**[44:04]** a deep impact. Once again check out the

**[44:04]** a deep impact. Once again check out the Microsoft work for good distillation

**[44:06]** Microsoft work for good distillation

**[44:06]** Microsoft work for good distillation scaling laws on this. Okay. Okay. Back

**[44:08]** scaling laws on this. Okay. Okay. Back

**[44:08]** scaling laws on this. Okay. Okay. Back to our paper. Instead of looking at our

**[44:10]** to our paper. Instead of looking at our

**[44:10]** to our paper. Instead of looking at our original um deepseek that's kind of the

**[44:14]** original um deepseek that's kind of the

**[44:14]** original um deepseek that's kind of the performance of where we're at

**[44:15]** performance of where we're at

**[44:15]** performance of where we're at distillation. Let's talk about these

**[44:17]** distillation. Let's talk about these

**[44:17]** distillation. Let's talk about these distillation models. So what we did was

**[44:19]** distillation models. So what we did was

**[44:19]** distillation models. So what we did was distill R1 down into llama and quen

**[44:21]** distill R1 down into llama and quen

**[44:22]** distill R1 down into llama and quen models. This is not RL. This is basic

**[44:25]** models. This is not RL. This is basic

**[44:25]** models. This is not RL. This is basic SFT. We have these models that reason

**[44:27]** SFT. We have these models that reason

**[44:27]** SFT. We have these models that reason for 25 30,000 steps. We take these

**[44:30]** for 25 30,000 steps. We take these

**[44:30]** for 25 30,000 steps. We take these traces, do SFT style distillation. So

**[44:33]** traces, do SFT style distillation. So

**[44:33]** traces, do SFT style distillation. So proper distillation, match your logits.

**[44:35]** proper distillation, match your logits.

**[44:35]** proper distillation, match your logits. And guess what? Um, this showed so much

**[44:39]** And guess what? Um, this showed so much

**[44:39]** And guess what? Um, this showed so much performance, but we know RL can do

**[44:41]** performance, but we know RL can do

**[44:41]** performance, but we know RL can do better. So, you know, all open source,

**[44:44]** better. So, you know, all open source,

**[44:44]** better. So, you know, all open source, all traces are open. Someone do RL based

**[44:47]** all traces are open. Someone do RL based

**[44:47]** all traces are open. Someone do RL based distillation from the big models. No one

**[44:49]** distillation from the big models. No one

**[44:49]** distillation from the big models. No one has done this as far as I know, but you

**[44:51]** has done this as far as I know, but you

**[44:51]** has done this as far as I know, but you know, we're able to get such good

**[44:53]** know, we're able to get such good

**[44:53]** know, we're able to get such good performance and there's still so much uh

**[44:55]** performance and there's still so much uh

**[44:55]** performance and there's still so much uh left to be done. But let's go over what

**[44:57]** left to be done. But let's go over what

**[44:57]** left to be done. But let's go over what we um distilled out. So these are the

**[44:59]** we um distilled out. So these are the

**[44:59]** we um distilled out. So these are the family of models. This is once again


### [45:00 - 46:00]

**[45:02]** family of models. This is once again

**[45:02]** family of models. This is once again pure SFT style distillation. Oh my

**[45:05]** pure SFT style distillation. Oh my

**[45:05]** pure SFT style distillation. Oh my slides are gone. They're back. Okay, so

**[45:08]** slides are gone. They're back. Okay, so

**[45:08]** slides are gone. They're back. Okay, so we distilled Quen 1.5B, Quen 7B, 14B,

**[45:12]** we distilled Quen 1.5B, Quen 7B, 14B,

**[45:12]** we distilled Quen 1.5B, Quen 7B, 14B, 32B, Llama 8B, 70B. Performance killed

**[45:16]** 32B, Llama 8B, 70B. Performance killed

**[45:16]** 32B, Llama 8B, 70B. Performance killed all the models they are themselves. So

**[45:18]** all the models they are themselves. So

**[45:18]** all the models they are themselves. So you take the model itself, you look at

**[45:20]** you take the model itself, you look at

**[45:20]** you take the model itself, you look at our distillation, it worked. Number went

**[45:23]** our distillation, it worked. Number went

**[45:23]** our distillation, it worked. Number went up like crazy. Really, really good

**[45:25]** up like crazy. Really, really good

**[45:25]** up like crazy. Really, really good performance. All our distills are now

**[45:28]** performance. All our distills are now

**[45:28]** performance. All our distills are now basically on par with GPT40. And for our

**[45:31]** basically on par with GPT40. And for our

**[45:31]** basically on par with GPT40. And for our new one, our new 8B distill is much much

**[45:34]** new one, our new 8B distill is much much

**[45:34]** new one, our new 8B distill is much much better. It's way better than 40. Um, and

**[45:36]** better. It's way better than 40. Um, and

**[45:36]** better. It's way better than 40. Um, and this is just once again RL and long

**[45:39]** this is just once again RL and long

**[45:39]** this is just once again RL and long chain of thought. Take that, do SFT

**[45:41]** chain of thought. Take that, do SFT

**[45:41]** chain of thought. Take that, do SFT style distillation. Um, question, what

**[45:44]** style distillation. Um, question, what

**[45:44]** style distillation. Um, question, what if we just did RL on the base model,

**[45:46]** if we just did RL on the base model,

**[45:46]** if we just did RL on the base model, right? We tried it. We tried RL on quen

**[45:49]** right? We tried it. We tried RL on quen

**[45:49]** right? We tried it. We tried RL on quen 32B for 10k steps. It's actually worse

**[45:51]** 32B for 10k steps. It's actually worse

**[45:52]** 32B for 10k steps. It's actually worse than distillation. So, you know, for

**[45:54]** than distillation. So, you know, for

**[45:54]** than distillation. So, you know, for small models, we don't want to just

**[45:55]** small models, we don't want to just

**[45:56]** small models, we don't want to just start with native RL. We saw this in our

**[45:58]** start with native RL. We saw this in our

**[45:58]** start with native RL. We saw this in our own model, right? We needed this cold

**[45:59]** own model, right? We needed this cold


### [46:00 - 47:00]

**[46:00]** own model, right? We needed this cold start. We need to kick off something

**[46:02]** start. We need to kick off something

**[46:02]** start. We need to kick off something from base models. R10 to actual R1. We

**[46:06]** from base models. R10 to actual R1. We

**[46:06]** from base models. R10 to actual R1. We had to do this SFT cold start. So, it

**[46:08]** had to do this SFT cold start. So, it

**[46:08]** had to do this SFT cold start. So, it actually performed significantly worse

**[46:09]** actually performed significantly worse

**[46:09]** actually performed significantly worse than distillation. Um, and of course,

**[46:11]** than distillation. Um, and of course,

**[46:11]** than distillation. Um, and of course, the Quinn team at the time, they had

**[46:13]** the Quinn team at the time, they had

**[46:13]** the Quinn team at the time, they had their own reasoning model, right? They

**[46:15]** their own reasoning model, right? They

**[46:15]** their own reasoning model, right? They had QWQ32B

**[46:17]** had QWQ32B

**[46:17]** had QWQ32B and um you know the R1 distill so the

**[46:19]** and um you know the R1 distill so the

**[46:19]** and um you know the R1 distill so the base model distill it did worse than

**[46:21]** base model distill it did worse than

**[46:21]** base model distill it did worse than what Quen did but kind of on par you

**[46:23]** what Quen did but kind of on par you

**[46:24]** what Quen did but kind of on par you know it's probably what they did our

**[46:26]** know it's probably what they did our

**[46:26]** know it's probably what they did our distillation on the uh chat model on the

**[46:29]** distillation on the uh chat model on the

**[46:29]** distillation on the uh chat model on the base chat model actually performed so

**[46:31]** base chat model actually performed so

**[46:31]** base chat model actually performed so much better we were we were able to do

**[46:33]** much better we were we were able to do

**[46:33]** much better we were we were able to do so much better than the Quen reasoning

**[46:35]** so much better than the Quen reasoning

**[46:35]** so much better than the Quen reasoning model and of course now we've taken this

**[46:37]** model and of course now we've taken this

**[46:37]** model and of course now we've taken this a step further with our new model okay

**[46:40]** a step further with our new model okay

**[46:40]** a step further with our new model okay future work R1 is worse than V3 at

**[46:42]** future work R1 is worse than V3 at

**[46:42]** future work R1 is worse than V3 at function calling multitap

**[46:44]** function calling multitap

**[46:44]** function calling multitap uh multi-turn and JSON mode. Guess what?

**[46:48]** uh multi-turn and JSON mode. Guess what?

**[46:48]** uh multi-turn and JSON mode. Guess what? Two weeks ago, there's a new DeepSeek

**[46:49]** Two weeks ago, there's a new DeepSeek

**[46:50]** Two weeks ago, there's a new DeepSeek model. We now do native function

**[46:51]** model. We now do native function

**[46:52]** model. We now do native function calling. We have JSON mode. We fixed it.

**[46:54]** calling. We have JSON mode. We fixed it.

**[46:54]** calling. We have JSON mode. We fixed it. It works. Um R1 struggles with language

**[46:57]** It works. Um R1 struggles with language

**[46:57]** It works. Um R1 struggles with language mixing. We don't know how we do on the

**[46:59]** mixing. We don't know how we do on the

**[46:59]** mixing. We don't know how we do on the new one. Uh it's sensitive to prompting.


### [47:00 - 48:00]

**[47:01]** new one. Uh it's sensitive to prompting.

**[47:01]** new one. Uh it's sensitive to prompting. I think this is something that the

**[47:02]** I think this is something that the

**[47:02]** I think this is something that the industry needs to figure out, right? Um

**[47:05]** industry needs to figure out, right? Um

**[47:05]** industry needs to figure out, right? Um spoiler alert for some latent space

**[47:07]** spoiler alert for some latent space

**[47:07]** spoiler alert for some latent space podcast fans, you know, we talked to

**[47:09]** podcast fans, you know, we talked to

**[47:09]** podcast fans, you know, we talked to some reasoning experts. So like some of

**[47:11]** some reasoning experts. So like some of

**[47:11]** some reasoning experts. So like some of the stuff that we're seeing, you know,

**[47:12]** the stuff that we're seeing, you know,

**[47:12]** the stuff that we're seeing, you know, um researchers at OpenAI, they're saying

**[47:15]** um researchers at OpenAI, they're saying

**[47:15]** um researchers at OpenAI, they're saying that, you know, if you're still doing

**[47:16]** that, you know, if you're still doing

**[47:16]** that, you know, if you're still doing scaffolding with reasoning models, we're

**[47:18]** scaffolding with reasoning models, we're

**[47:18]** scaffolding with reasoning models, we're failing as labs. So, you know, we need

**[47:21]** failing as labs. So, you know, we need

**[47:21]** failing as labs. So, you know, we need to learn how to prompt these things

**[47:22]** to learn how to prompt these things

**[47:22]** to learn how to prompt these things better. Um and it's not much better at

**[47:25]** better. Um and it's not much better at

**[47:25]** better. Um and it's not much better at engineering tasks than V3. That's all

**[47:27]** engineering tasks than V3. That's all

**[47:27]** engineering tasks than V3. That's all fake. This is old news. This was our old

**[47:29]** fake. This is old news. This was our old

**[47:29]** fake. This is old news. This was our old DeepSeek. The new DeepSeek is a lot

**[47:31]** DeepSeek. The new DeepSeek is a lot

**[47:31]** DeepSeek. The new DeepSeek is a lot better. Um open recreations. We want to

**[47:34]** better. Um open recreations. We want to

**[47:34]** better. Um open recreations. We want to promote open research, right? So there

**[47:36]** promote open research, right? So there

**[47:36]** promote open research, right? So there were people trying to recreate this. Uh,

**[47:38]** were people trying to recreate this. Uh,

**[47:38]** were people trying to recreate this. Uh, Hugging Face has a version of this. Um,

**[47:40]** Hugging Face has a version of this. Um,

**[47:40]** Hugging Face has a version of this. Um, Bespoke Labs was doing this. I don't

**[47:42]** Bespoke Labs was doing this. I don't

**[47:42]** Bespoke Labs was doing this. I don't know how they're doing. Uh, there's

**[47:43]** know how they're doing. Uh, there's

**[47:43]** know how they're doing. Uh, there's quite a few people now that have done

**[47:45]** quite a few people now that have done

**[47:45]** quite a few people now that have done this. But yeah, that's kind of an

**[47:47]** this. But yeah, that's kind of an

**[47:47]** this. But yeah, that's kind of an overview. I know a lot more people have

**[47:49]** overview. I know a lot more people have

**[47:49]** overview. I know a lot more people have joined. We now have like 100 200 people

**[47:51]** joined. We now have like 100 200 people

**[47:51]** joined. We now have like 100 200 people in the audience. So, I'm going to do a

**[47:53]** in the audience. So, I'm going to do a

**[47:53]** in the audience. So, I'm going to do a quick recap in our last 10 minutes. So,

**[47:55]** quick recap in our last 10 minutes. So,

**[47:56]** quick recap in our last 10 minutes. So, um, first we are launching a second

**[47:58]** um, first we are launching a second

**[47:58]** um, first we are launching a second paper club. Um, every year, uh, every


### [48:00 - 49:00]

**[48:01]** paper club. Um, every year, uh, every

**[48:02]** paper club. Um, every year, uh, every week we do our normal paper club where

**[48:03]** week we do our normal paper club where

**[48:03]** week we do our normal paper club where we take the latest paper. We have a

**[48:05]** we take the latest paper. We have a

**[48:05]** we take the latest paper. We have a hundred people that join every week, 300

**[48:08]** hundred people that join every week, 300

**[48:08]** hundred people that join every week, 300 for DeepSeek. We're we're turning this

**[48:10]** for DeepSeek. We're we're turning this

**[48:10]** for DeepSeek. We're we're turning this into a test of time paper club. If

**[48:13]** into a test of time paper club. If

**[48:13]** into a test of time paper club. If you're interested, sign up. Uh we're

**[48:15]** you're interested, sign up. Uh we're

**[48:15]** you're interested, sign up. Uh we're going to run this in SF and we're going

**[48:17]** going to run this in SF and we're going

**[48:17]** going to run this in SF and we're going to do it remotely over the next 6

**[48:19]** to do it remotely over the next 6

**[48:19]** to do it remotely over the next 6 months. We're going to cover 50 to 100

**[48:21]** months. We're going to cover 50 to 100

**[48:21]** months. We're going to cover 50 to 100 papers. We're going to break up what you

**[48:23]** papers. We're going to break up what you

**[48:23]** papers. We're going to break up what you would need to know as an AI engineer

**[48:25]** would need to know as an AI engineer

**[48:25]** would need to know as an AI engineer into different buckets. So stuff like

**[48:27]** into different buckets. So stuff like

**[48:27]** into different buckets. So stuff like you know what are foundations of deep

**[48:29]** you know what are foundations of deep

**[48:29]** you know what are foundations of deep learning attention RL um optimizers atom

**[48:34]** learning attention RL um optimizers atom

**[48:34]** learning attention RL um optimizers atom gradient descent foundation models that

**[48:36]** gradient descent foundation models that

**[48:36]** gradient descent foundation models that you should know about GPT2 BERT RNN's

**[48:39]** you should know about GPT2 BERT RNN's

**[48:39]** you should know about GPT2 BERT RNN's LSTMs pre-training post- training

**[48:41]** LSTMs pre-training post- training

**[48:41]** LSTMs pre-training post- training mid-training so scaling laws chinchilla

**[48:44]** mid-training so scaling laws chinchilla

**[48:44]** mid-training so scaling laws chinchilla distillation we'll cover days of

**[48:47]** distillation we'll cover days of

**[48:47]** distillation we'll cover days of diffusion optimization voice fine-tuning

**[48:50]** diffusion optimization voice fine-tuning

**[48:50]** diffusion optimization voice fine-tuning we basically have a paper club where

**[48:52]** we basically have a paper club where

**[48:52]** we basically have a paper club where every week we're going to split up these

**[48:55]** every week we're going to split up these

**[48:55]** every week we're going to split up these core concepts into a few papers. We'll

**[48:57]** core concepts into a few papers. We'll

**[48:58]** core concepts into a few papers. We'll have a presentation of three to four

**[48:59]** have a presentation of three to four

**[48:59]** have a presentation of three to four papers. Everyone is welcome to join.


### [49:00 - 50:00]

**[49:02]** papers. Everyone is welcome to join.

**[49:02]** papers. Everyone is welcome to join. We're going to have a presentation on

**[49:03]** We're going to have a presentation on

**[49:03]** We're going to have a presentation on every core concept and then open

**[49:05]** every core concept and then open

**[49:05]** every core concept and then open discussion. This is not a course is

**[49:07]** discussion. This is not a course is

**[49:07]** discussion. This is not a course is courses are good. You know, you have

**[49:09]** courses are good. You know, you have

**[49:09]** courses are good. You know, you have active workshop, you build stuff, you

**[49:11]** active workshop, you build stuff, you

**[49:11]** active workshop, you build stuff, you you actually like do active learning.

**[49:13]** you actually like do active learning.

**[49:13]** you actually like do active learning. This is still a paper club, you know.

**[49:15]** This is still a paper club, you know.

**[49:15]** This is still a paper club, you know. This is if you want to know the

**[49:17]** This is if you want to know the

**[49:17]** This is if you want to know the foundations of what's going on under the

**[49:19]** foundations of what's going on under the

**[49:19]** foundations of what's going on under the hood. These are the key papers to know.

**[49:21]** hood. These are the key papers to know.

**[49:21]** hood. These are the key papers to know. We'll invite a lot of speakers. We'll

**[49:22]** We'll invite a lot of speakers. We'll

**[49:22]** We'll invite a lot of speakers. We'll have people present. We'll have good

**[49:24]** have people present. We'll have good

**[49:24]** have people present. We'll have good discussions. But yeah, test of time

**[49:26]** discussions. But yeah, test of time

**[49:26]** discussions. But yeah, test of time paper club coming in June. Scan QR code.

**[49:30]** paper club coming in June. Scan QR code.

**[49:30]** paper club coming in June. Scan QR code. Let us know if you want to be involved,

**[49:32]** Let us know if you want to be involved,

**[49:32]** Let us know if you want to be involved, if you want to recommend a paper, share

**[49:34]** if you want to recommend a paper, share

**[49:34]** if you want to recommend a paper, share a paper. Um, we'll share curriculum

**[49:37]** a paper. Um, we'll share curriculum

**[49:37]** a paper. Um, we'll share curriculum soon. Join the latent space uh discord.

**[49:40]** soon. Join the latent space uh discord.

**[49:40]** soon. Join the latent space uh discord. We already have a list of top 2025

**[49:43]** We already have a list of top 2025

**[49:43]** We already have a list of top 2025 papers. It's a paper every week that you

**[49:45]** papers. It's a paper every week that you

**[49:46]** papers. It's a paper every week that you can go through. We're going to we're

**[49:47]** can go through. We're going to we're

**[49:47]** can go through. We're going to we're going to build off of that. And once

**[49:48]** going to build off of that. And once

**[49:48]** going to build off of that. And once again, final recap. What did we talk

**[49:52]** again, final recap. What did we talk

**[49:52]** again, final recap. What did we talk about today? Today we talked about the

**[49:54]** about today? Today we talked about the

**[49:54]** about today? Today we talked about the new DeepSseek model. So two weeks ago,

**[49:57]** new DeepSseek model. So two weeks ago,

**[49:57]** new DeepSseek model. So two weeks ago, DeepSseek R1 May 28 came out. Basically,


### [50:00 - 51:00]

**[50:01]** DeepSseek R1 May 28 came out. Basically,

**[50:01]** DeepSseek R1 May 28 came out. Basically, Deepseek took the last DeepSseek model

**[50:03]** Deepseek took the last DeepSseek model

**[50:03]** Deepseek took the last DeepSseek model that was as good as 01. We trained it to

**[50:06]** that was as good as 01. We trained it to

**[50:06]** that was as good as 01. We trained it to reason for twice as long. We got

**[50:08]** reason for twice as long. We got

**[50:08]** reason for twice as long. We got significantly better performance.

**[50:10]** significantly better performance.

**[50:10]** significantly better performance. Deepseek R1 May 28th can now do standard

**[50:15]** Deepseek R1 May 28th can now do standard

**[50:15]** Deepseek R1 May 28th can now do standard structured JSON output native function

**[50:17]** structured JSON output native function

**[50:17]** structured JSON output native function calling hallucinates less reasons for

**[50:20]** calling hallucinates less reasons for

**[50:20]** calling hallucinates less reasons for twice as long and is a much much better

**[50:23]** twice as long and is a much much better

**[50:23]** twice as long and is a much much better jump in performance from being 01 level.

**[50:26]** jump in performance from being 01 level.

**[50:26]** jump in performance from being 01 level. Deepseek is now on par with OpenAI's 03

**[50:30]** Deepseek is now on par with OpenAI's 03

**[50:30]** Deepseek is now on par with OpenAI's 03 model and Gemini 2.5. Basically across

**[50:33]** model and Gemini 2.5. Basically across

**[50:33]** model and Gemini 2.5. Basically across the board on all benchmarks we are now

**[50:35]** the board on all benchmarks we are now

**[50:35]** the board on all benchmarks we are now as good as um Gemini 25 and OpenAI's 03.

**[50:39]** as good as um Gemini 25 and OpenAI's 03.

**[50:39]** as good as um Gemini 25 and OpenAI's 03. The other model that was released is um

**[50:42]** The other model that was released is um

**[50:42]** The other model that was released is um DeepSeek R1 Quen 3 8B. So we took Quen 3

**[50:47]** DeepSeek R1 Quen 3 8B. So we took Quen 3

**[50:47]** DeepSeek R1 Quen 3 8B. So we took Quen 3 8B distilled it down into a reasoning

**[50:50]** 8B distilled it down into a reasoning

**[50:50]** 8B distilled it down into a reasoning model based on our longer traces and we

**[50:53]** model based on our longer traces and we

**[50:53]** model based on our longer traces and we killed it. So it's a small 8B where we

**[50:56]** killed it. So it's a small 8B where we

**[50:56]** killed it. So it's a small 8B where we do post training via distillation as SFT

**[50:59]** do post training via distillation as SFT

**[50:59]** do post training via distillation as SFT on reasoning traces and model got really


### [51:00 - 52:00]

**[51:01]** on reasoning traces and model got really

**[51:01]** on reasoning traces and model got really good. You take base quen 38B and you

**[51:04]** good. You take base quen 38B and you

**[51:04]** good. You take base quen 38B and you take R quen 38B it's very good. Uh

**[51:07]** take R quen 38B it's very good. Uh

**[51:07]** take R quen 38B it's very good. Uh looking at the benchmarks here, you

**[51:08]** looking at the benchmarks here, you

**[51:08]** looking at the benchmarks here, you know, our opensource ondevice runnable

**[51:12]** know, our opensource ondevice runnable

**[51:12]** know, our opensource ondevice runnable Quen 38B non-reasoning model is on par

**[51:17]** Quen 38B non-reasoning model is on par

**[51:17]** Quen 38B non-reasoning model is on par with Gemini 2.5 flash thinking 03 mini

**[51:22]** with Gemini 2.5 flash thinking 03 mini

**[51:22]** with Gemini 2.5 flash thinking 03 mini better than 54, significantly better

**[51:25]** better than 54, significantly better

**[51:25]** better than 54, significantly better than Quen 38B. Our 8B reasoner is better

**[51:29]** than Quen 38B. Our 8B reasoner is better

**[51:29]** than Quen 38B. Our 8B reasoner is better than Quen 32B. It's on par with Quen's

**[51:33]** than Quen 32B. It's on par with Quen's

**[51:33]** than Quen 32B. It's on par with Quen's 235B reasoning model. Um so you know two

**[51:36]** 235B reasoning model. Um so you know two

**[51:36]** 235B reasoning model. Um so you know two major updates new reasoning model from

**[51:39]** major updates new reasoning model from

**[51:39]** major updates new reasoning model from deepseek as good as um 03 and Gemini 2.5

**[51:43]** deepseek as good as um 03 and Gemini 2.5

**[51:43]** deepseek as good as um 03 and Gemini 2.5 new mini 8B model that is as good as 2.5

**[51:46]** new mini 8B model that is as good as 2.5

**[51:46]** new mini 8B model that is as good as 2.5 thinking and 03 mini of course all open

**[51:49]** thinking and 03 mini of course all open

**[51:49]** thinking and 03 mini of course all open source run on your laptop um just as

**[51:52]** source run on your laptop um just as

**[51:52]** source run on your laptop um just as good you know and these are not even R2

**[51:55]** good you know and these are not even R2

**[51:55]** good you know and these are not even R2 this is not DeepSec R2 this is our mini

**[51:58]** this is not DeepSec R2 this is our mini

**[51:58]** this is not DeepSec R2 this is our mini title refresh with a new date um yeah so


### [52:00 - 53:00]

**[52:02]** title refresh with a new date um yeah so

**[52:02]** title refresh with a new date um yeah so high level that's what we talked about

**[52:04]** high level that's what we talked about

**[52:04]** high level that's what we talked about um you we see aha moments. Instead of um

**[52:08]** um you we see aha moments. Instead of um

**[52:08]** um you we see aha moments. Instead of um training on next token prediction, we

**[52:10]** training on next token prediction, we

**[52:10]** training on next token prediction, we scale this out to inference time

**[52:12]** scale this out to inference time

**[52:12]** scale this out to inference time scaling. So we now train models to train

**[52:15]** scaling. So we now train models to train

**[52:15]** scaling. So we now train models to train and think for longer. We get aha

**[52:17]** and think for longer. We get aha

**[52:17]** and think for longer. We get aha moments. And that's kind of the update

**[52:19]** moments. And that's kind of the update

**[52:19]** moments. And that's kind of the update to our new um to our new DeepSec models.

**[52:23]** to our new um to our new DeepSec models.

**[52:23]** to our new um to our new DeepSec models. So yeah, thanks everyone for coming.

**[52:25]** So yeah, thanks everyone for coming.

**[52:25]** So yeah, thanks everyone for coming. Thanks for listening. Join paper club.

**[52:28]** Thanks for listening. Join paper club.

**[52:28]** Thanks for listening. Join paper club. [Applause]

**[52:34]** [Applause]

**[52:34]** [Applause] Who is this? Oh yeah. Um, so a lot of

**[52:36]** Who is this? Oh yeah. Um, so a lot of

**[52:36]** Who is this? Oh yeah. Um, so a lot of our regulars that help make paper club

**[52:38]** our regulars that help make paper club

**[52:38]** our regulars that help make paper club are here. Uh, Eugene, Era, R.J. Flo is

**[52:42]** are here. Uh, Eugene, Era, R.J. Flo is

**[52:42]** are here. Uh, Eugene, Era, R.J. Flo is here. Um, if anyone else is a regular in

**[52:45]** here. Um, if anyone else is a regular in

**[52:45]** here. Um, if anyone else is a regular in paper club, you know, come up. Uh, every

**[52:47]** paper club, you know, come up. Uh, every

**[52:47]** paper club, you know, come up. Uh, every week we have our weekly paper club.

**[52:49]** week we have our weekly paper club.

**[52:49]** week we have our weekly paper club. These are these are the homies that make

**[52:51]** These are these are the homies that make

**[52:51]** These are these are the homies that make it possible. Uh, we're going to have our

**[52:53]** it possible. Uh, we're going to have our

**[52:53]** it possible. Uh, we're going to have our second paper club. Test of time. We'll

**[52:55]** second paper club. Test of time. We'll

**[52:55]** second paper club. Test of time. We'll be there soon. But yeah, uh, you know,

**[52:57]** be there soon. But yeah, uh, you know,

**[52:57]** be there soon. But yeah, uh, you know, major shout out. This is not this is not


### [53:00 - 54:00]

**[53:00]** major shout out. This is not this is not

**[53:00]** major shout out. This is not this is not me. This is not Swixs. This is

**[53:02]** me. This is not Swixs. This is

**[53:02]** me. This is not Swixs. This is volunteers and more on Zoom. Every week

**[53:05]** volunteers and more on Zoom. Every week

**[53:05]** volunteers and more on Zoom. Every week on a weekday at noon, hundreds of you

**[53:08]** on a weekday at noon, hundreds of you

**[53:08]** on a weekday at noon, hundreds of you join in to discuss a paper. So, you

**[53:10]** join in to discuss a paper. So, you

**[53:10]** join in to discuss a paper. So, you know, big shout out to everyone here.

**[53:12]** know, big shout out to everyone here.

**[53:12]** know, big shout out to everyone here. Not possible without us. You know, all

**[53:13]** Not possible without us. You know, all

**[53:13]** Not possible without us. You know, all the authors as well that have come, all

**[53:15]** the authors as well that have come, all

**[53:15]** the authors as well that have come, all the authors that have been able to

**[53:17]** the authors that have been able to

**[53:17]** the authors that have been able to share. Let's just give it up for

**[53:18]** share. Let's just give it up for

**[53:18]** share. Let's just give it up for everyone that makes a paper club

**[53:19]** everyone that makes a paper club

**[53:19]** everyone that makes a paper club possible.

**[53:22]** possible.

**[53:22]** possible. Eugene Yan who is over there running his

**[53:24]** Eugene Yan who is over there running his

**[53:24]** Eugene Yan who is over there running his track. Sam I know is speaking right now.

**[53:27]** track. Sam I know is speaking right now.

**[53:27]** track. Sam I know is speaking right now. Six who is putting all this on. Um Yeah,

**[53:32]** Six who is putting all this on. Um Yeah,

**[53:32]** Six who is putting all this on. Um Yeah, of course. Um, once again, I'll leave

**[53:34]** of course. Um, once again, I'll leave

**[53:34]** of course. Um, once again, I'll leave our QR code here. If you're interested

**[53:36]** our QR code here. If you're interested

**[53:36]** our QR code here. If you're interested in test of time volunteering for a

**[53:38]** in test of time volunteering for a

**[53:38]** in test of time volunteering for a paper, recommending a paper, fill it

**[53:40]** paper, recommending a paper, fill it

**[53:40]** paper, recommending a paper, fill it out, you know, help us out. This is our

**[53:42]** out, you know, help us out. This is our

**[53:42]** out, you know, help us out. This is our paper club selfie. But yeah, thanks for

**[53:44]** paper club selfie. But yeah, thanks for

**[53:44]** paper club selfie. But yeah, thanks for coming out everyone. Enjoy the rest of

**[53:46]** coming out everyone. Enjoy the rest of

**[53:46]** coming out everyone. Enjoy the rest of the conference.

**[53:47]** the conference.

**[53:47]** the conference. [Music]


